{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from instances import (\n",
    "    generate_copying_instance,\n",
    "    generate_selective_copying_instance,\n",
    "    generate_in_context_recall_instance,\n",
    "    generate_noisy_in_context_recall_instance,\n",
    "    generate_vocab_permutations,\n",
    "    generate_fuzzy_in_context_recall_instance,\n",
    "    generate_kv_map,\n",
    "    generate_memorization_instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target ignore token = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 3, 0, 5, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6]),\n",
       " array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100,    1,    3,    0,    5,    3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = generate_copying_instance(vocab_size=8, seq_len=20, num_tokens_to_copy=5)\n",
    "inputs, targets\n",
    "# copy token = 7\n",
    "# blank token = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6, 6, 6, 5, 6, 6, 0, 2, 6, 6, 6, 6, 0, 3, 7, 6, 6, 6, 6, 6]),\n",
       " array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100,    5,    0,    2,    0,    3]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = generate_selective_copying_instance(vocab_size=8, seq_len=20, num_tokens_to_copy=5)\n",
    "inputs, targets\n",
    "# copy token = 7\n",
    "# blank token = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  5,  1,  9,  1,  9,  2,  8,  0,  5,  2,  8,  0,  5,  3,  5,  3,\n",
       "         5, 11,  0]),\n",
       " array([ 5,  1,  9,  1,  9,  2,  8,  0,  5,  2,  8,  0,  5,  3,  5,  3,  5,\n",
       "        11,  0,  5]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, shifted_inputs = generate_in_context_recall_instance(vocab_size=12, seq_len=20, is_training=True, noise_vocab_size=0)\n",
    "inputs, shifted_inputs\n",
    "# copy token = 11\n",
    "# key vocab [0, 1, 2, 3, 4]\n",
    "# value vocab [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3,  7,  4, 10,  0, 10,  0, 10,  3,  7,  2,  9,  3,  7,  2,  9,  1,\n",
       "        10, 11,  0]),\n",
       " array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100,   10]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = generate_in_context_recall_instance(vocab_size=12, seq_len=20, is_training=False, noise_vocab_size=0)\n",
    "inputs, targets\n",
    "# copy token = 11\n",
    "# key vocab [0, 1, 2, 3, 4]\n",
    "# value vocab [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  7, 13, 12,  0,  8, 12, 14,  1,  9,  1,  9,  0,  8, 13, 14,  3,\n",
       "        10, 15,  3]),\n",
       " array([ 7, 13, 12,  0,  8, 12, 14,  1,  9,  1,  9,  0,  8, 13, 14,  3, 10,\n",
       "        15,  3, 10]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, shifted_inputs = generate_noisy_in_context_recall_instance(vocab_size=16, seq_len=20, is_training=True, noise_vocab_size=3)\n",
    "inputs, shifted_inputs\n",
    "# copy token = 15\n",
    "# key vocab [0, 1, 2, 3, 4]\n",
    "# value vocab [5, 6, 7, 8, 9]\n",
    "# noise vocab [12, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  8,  3, 11,  3, 11,  0,  8,  0,  8,  5, 11,  5, 11,  3, 11,  0,\n",
       "         8, 15,  0]),\n",
       " array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100,    8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = generate_noisy_in_context_recall_instance(vocab_size=16, seq_len=20, is_training=False, noise_vocab_size=3)\n",
    "inputs, targets\n",
    "# copy token = 15\n",
    "# key vocab [0, 1, 2, 3, 4]\n",
    "# value vocab [5, 6, 7, 8, 9]\n",
    "# noise vocab [12, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(0),),\n",
       " (np.int64(1),),\n",
       " (np.int64(2),),\n",
       " (np.int64(3),),\n",
       " (np.int64(4),),\n",
       " (np.int64(5),),\n",
       " (np.int64(6),),\n",
       " (np.int64(7),)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.arange(8)\n",
    "generate_vocab_permutations(vocab, token_motif_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(0), np.int64(1)),\n",
       " (np.int64(0), np.int64(2)),\n",
       " (np.int64(0), np.int64(3)),\n",
       " (np.int64(1), np.int64(0)),\n",
       " (np.int64(1), np.int64(2)),\n",
       " (np.int64(1), np.int64(3)),\n",
       " (np.int64(2), np.int64(0)),\n",
       " (np.int64(2), np.int64(1)),\n",
       " (np.int64(2), np.int64(3)),\n",
       " (np.int64(3), np.int64(0)),\n",
       " (np.int64(3), np.int64(1)),\n",
       " (np.int64(3), np.int64(2))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.arange(4)\n",
    "generate_vocab_permutations(vocab, token_motif_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10, 10,  0,  3,  8,  6,  9,  0,  9,  7,  0,  2,  9,  8,  5, 11,  0,\n",
       "         3,  8,  6]),\n",
       " array([10,  0,  3,  8,  6,  9,  0,  9,  7,  0,  2,  9,  8,  5, 11,  0,  3,\n",
       "         8,  6,  9]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, shifted_inputs = generate_fuzzy_in_context_recall_instance(vocab_size=12, seq_len=20, k_motif_size=3, v_motif_size=3, is_training=True, noise_vocab_size=0)\n",
    "inputs, shifted_inputs\n",
    "# copy token = 11\n",
    "# pad token = 10\n",
    "# key vocab [0, 1, 2, 3, 4]\n",
    "# value vocab [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10, 10, 10, 10, 10,  2,  4,  1,  9,  8,  3,  0,  2,  9,  6, 11,  3,\n",
       "         0,  2,  9]),\n",
       " array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100,    9,    6]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = generate_fuzzy_in_context_recall_instance(vocab_size=12, seq_len=20, k_motif_size=3, v_motif_size=3, is_training=False, noise_vocab_size=0)\n",
    "inputs, targets\n",
    "# copy token = 11\n",
    "# pad token = 10\n",
    "# key vocab [0, 1, 2, 3, 4]\n",
    "# value vocab [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(np.int64(2),): (np.int64(6),),\n",
       " (np.int64(0),): (np.int64(4),),\n",
       " (np.int64(3),): (np.int64(7),),\n",
       " (np.int64(1),): (np.int64(5),)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_kv_map(vocab_size=8, k_motif_size=1, v_motif_size=1)\n",
    "# key vocab [0, 1, 2, 3]\n",
    "# value vocab [4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 7, 1, 7, 2, 7, 1, 7, 0, 7, 1, 7, 0, 7, 1, 7, 0, 7, 2, 7]),\n",
       " array([-100,    4, -100,    4, -100,    6, -100,    4, -100,    3, -100,\n",
       "           4, -100,    3, -100,    4, -100,    3, -100,    6]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = generate_memorization_instance(vocab_size=8, seq_len=20)\n",
    "inputs, targets\n",
    "# insert token = 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
