# Training

IMDB sequences have token_id length varies between 100 to 500. Truncating text to 

For Hybrid pretrained model, GPU memory usage reaches upto 12000 MB. 

Use Wandb. 