# Results


## Memory Usage

Mamba 130m backbone-freezed fine-tuning
* IMDB batch size 1 : 2200 MB

Hybrid GPT-Neo + Mamba 130m backbone-freezed fine-tuning
* IMDB batch size 1: 9800 MB

## IMDB Dataset 

**Mamba**
Config: epoch = 1, lr = 5e-5, batch_size = 1, train_size 5000 eval_size 200

