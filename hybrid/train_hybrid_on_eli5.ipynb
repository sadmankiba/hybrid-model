{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (0.25.2)\n",
      "Requirement already satisfied: filelock in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /u/r/i/riyad/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, MambaForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "from hybrid_model import HybridModel\n",
    "from datasets import load_dataset\n",
    "# from transformers import MambaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed, buffer_size = 42, 10_000\n",
    "# dataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\n",
    "# dataset = dataset.shuffle(seed, buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M',padding_side=\"left\")\n",
    "# if transformer_tokenizer.mask_token is None:\n",
    "#     transformer_tokenizer.add_special_tokens({'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.with_format(\"torch\")\n",
    "dataset = load_dataset(\"eli5_category\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformer_tokenizer.pad_token is None:\n",
    "    #transformer_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    transformer_tokenizer.pad_token = transformer_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed the dataset to eli5  It's an English-language dataset of questions and answers gathered from the \n",
    "# r/explainlikeimfive subreddit where users ask factual questions requiring paragraph-length or longer answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length =  100\n",
    "def tokenize(example):\n",
    "    return transformer_tokenizer(example, truncation=True, max_length=max_seq_length, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    result = tokenize([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 73417/73417 [00:45<00:00, 1602.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 73417\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=transformer_tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(tokenized_eli5, batch_size=8, collate_fn=data_collator,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.cuda.device(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'EleutherAI/gpt-neo-125M'\n",
    ")\n",
    "mamba_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "mamba_model = mamba_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "Parameter containing:\n",
      "tensor([1.3569], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in transformer_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in mamba_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# model.to(device) ? unfreeze : not\n",
    "hybrid_model = HybridModel(transformer_model=transformer_model.transformer, mamba_model=mamba_model.backbone,proj_type= \"gressf\", n_hybrid_blocks=2).to(device)\n",
    "trainer_params = []\n",
    "# for combiner in model.combiners.parameters():\n",
    "#     trainer_params.append(combiner)\n",
    "\n",
    "# for spliter in model.splitters.parameters():\n",
    "#     trainer_params.append(spliter)\n",
    "\n",
    "for param in hybrid_model.parameters():\n",
    "    if param.requires_grad:\n",
    "       trainer_params.append(param)\n",
    "    \n",
    "#trainer_params = torch.T(trainer_params)\n",
    "print(trainer_params[0])\n",
    "len(trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([1])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([50257, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for param in trainer_params:\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 120.59175109863281\n",
      "loss: 107.79784393310547\n",
      "loss: 104.14891052246094\n",
      "loss: 100.83552551269531\n",
      "loss: 98.62007141113281\n",
      "loss: 92.268798828125\n",
      "loss: 92.0339584350586\n",
      "loss: 88.72013854980469\n",
      "loss: 86.01180267333984\n",
      "loss: 84.80916595458984\n",
      "loss: 82.48648071289062\n",
      "loss: 79.53721618652344\n",
      "loss: 78.9300765991211\n",
      "loss: 76.74812316894531\n",
      "loss: 75.66899108886719\n",
      "loss: 71.90830993652344\n",
      "loss: 73.82843780517578\n",
      "loss: 71.40892791748047\n",
      "loss: 69.50786590576172\n",
      "loss: 66.57719421386719\n",
      "loss: 67.7821044921875\n",
      "loss: 65.99663543701172\n",
      "loss: 66.26202392578125\n",
      "loss: 64.588134765625\n",
      "loss: 62.754669189453125\n",
      "loss: 60.79283905029297\n",
      "loss: 61.52482223510742\n",
      "loss: 61.24024200439453\n",
      "loss: 61.829978942871094\n",
      "loss: 59.567996978759766\n",
      "loss: 57.48764419555664\n",
      "loss: 57.59124755859375\n",
      "loss: 57.535152435302734\n",
      "loss: 57.88046646118164\n",
      "loss: 57.814266204833984\n",
      "loss: 54.71114730834961\n",
      "loss: 55.82950973510742\n",
      "loss: 55.323822021484375\n",
      "loss: 53.26999282836914\n",
      "loss: 56.11664962768555\n",
      "loss: 52.3409423828125\n",
      "loss: 52.54449462890625\n",
      "loss: 50.72429275512695\n",
      "loss: 53.11848831176758\n",
      "loss: 50.369110107421875\n",
      "loss: 52.808204650878906\n",
      "loss: 50.07546615600586\n",
      "loss: 50.857913970947266\n",
      "loss: 52.10528564453125\n",
      "loss: 48.75380325317383\n",
      "loss: 49.677738189697266\n",
      "loss: 47.521366119384766\n",
      "loss: 47.96615982055664\n",
      "loss: 48.01697540283203\n",
      "loss: 51.55464553833008\n",
      "loss: 49.81377410888672\n",
      "loss: 46.0322265625\n",
      "loss: 47.45546340942383\n",
      "loss: 46.466766357421875\n",
      "loss: 48.312862396240234\n",
      "loss: 45.657291412353516\n",
      "loss: 47.31888198852539\n",
      "loss: 47.2099494934082\n",
      "loss: 45.500518798828125\n",
      "loss: 46.8409538269043\n",
      "loss: 44.50840759277344\n",
      "loss: 43.29971694946289\n",
      "loss: 44.83894348144531\n",
      "loss: 47.105228424072266\n",
      "loss: 42.383914947509766\n",
      "loss: 45.47012710571289\n",
      "loss: 42.17401885986328\n",
      "loss: 45.5320930480957\n",
      "loss: 41.60895538330078\n",
      "loss: 39.74140548706055\n",
      "loss: 42.71767807006836\n",
      "loss: 43.37624740600586\n",
      "loss: 42.3989372253418\n",
      "loss: 42.415706634521484\n",
      "loss: 41.254150390625\n",
      "loss: 43.89765548706055\n",
      "loss: 43.040069580078125\n",
      "loss: 42.042076110839844\n",
      "loss: 43.0514030456543\n",
      "loss: 42.04216766357422\n",
      "loss: 40.380489349365234\n",
      "loss: 42.151859283447266\n",
      "loss: 43.442970275878906\n",
      "loss: 40.06346130371094\n",
      "loss: 39.72312927246094\n",
      "loss: 42.0609130859375\n",
      "loss: 40.986961364746094\n",
      "loss: 38.93841552734375\n",
      "loss: 38.269874572753906\n",
      "loss: 42.375064849853516\n",
      "loss: 40.226844787597656\n",
      "loss: 38.878170013427734\n",
      "loss: 40.23192596435547\n",
      "loss: 39.905216217041016\n",
      "loss: 39.017024993896484\n",
      "loss: 43.42265701293945\n",
      "loss: 41.21568298339844\n",
      "loss: 38.87191390991211\n",
      "loss: 40.43408203125\n",
      "loss: 37.890907287597656\n",
      "loss: 39.415321350097656\n",
      "loss: 36.73040008544922\n",
      "loss: 35.66547393798828\n",
      "loss: 39.045387268066406\n",
      "loss: 38.782962799072266\n",
      "loss: 34.43315505981445\n",
      "loss: 37.89710235595703\n",
      "loss: 40.22834014892578\n",
      "loss: 34.85498046875\n",
      "loss: 35.26758575439453\n",
      "loss: 39.03046417236328\n",
      "loss: 39.557796478271484\n",
      "loss: 37.02275466918945\n",
      "loss: 35.352142333984375\n",
      "loss: 35.517799377441406\n",
      "loss: 37.36865997314453\n",
      "loss: 35.872032165527344\n",
      "loss: 36.967777252197266\n",
      "loss: 38.319183349609375\n",
      "loss: 35.053253173828125\n",
      "loss: 35.68433380126953\n",
      "loss: 36.70915985107422\n",
      "loss: 36.59859085083008\n",
      "loss: 35.487483978271484\n",
      "loss: 36.91523742675781\n",
      "loss: 34.953643798828125\n",
      "loss: 35.068389892578125\n",
      "loss: 34.999366760253906\n",
      "loss: 37.71434783935547\n",
      "loss: 35.84757995605469\n",
      "loss: 35.73158264160156\n",
      "loss: 38.111942291259766\n",
      "loss: 35.44416046142578\n",
      "loss: 33.622535705566406\n",
      "loss: 34.919437408447266\n",
      "loss: 34.479408264160156\n",
      "loss: 37.870723724365234\n",
      "loss: 35.94666290283203\n",
      "loss: 35.33413314819336\n",
      "loss: 37.48768997192383\n",
      "loss: 34.693443298339844\n",
      "loss: 37.22450637817383\n",
      "loss: 35.53525924682617\n",
      "loss: 34.73421859741211\n",
      "loss: 34.319740295410156\n",
      "loss: 36.095306396484375\n",
      "loss: 34.619022369384766\n",
      "loss: 35.74042892456055\n",
      "loss: 34.35226058959961\n",
      "loss: 36.93402099609375\n",
      "loss: 34.23351287841797\n",
      "loss: 34.28472137451172\n",
      "loss: 33.506587982177734\n",
      "loss: 35.65833282470703\n",
      "loss: 33.400611877441406\n",
      "loss: 33.02097702026367\n",
      "loss: 33.58372116088867\n",
      "loss: 33.14012145996094\n",
      "loss: 32.96176528930664\n",
      "loss: 33.257144927978516\n",
      "loss: 32.23693084716797\n",
      "loss: 32.3500862121582\n",
      "loss: 34.62787628173828\n",
      "loss: 34.91822052001953\n",
      "loss: 35.21357727050781\n",
      "loss: 32.32910919189453\n",
      "loss: 32.33517837524414\n",
      "loss: 31.923965454101562\n",
      "loss: 34.62067794799805\n",
      "loss: 30.666208267211914\n",
      "loss: 32.60273742675781\n",
      "loss: 33.071319580078125\n",
      "loss: 31.30950164794922\n",
      "loss: 36.314170837402344\n",
      "loss: 31.716550827026367\n",
      "loss: 36.657371520996094\n",
      "loss: 30.21163558959961\n",
      "loss: 33.459686279296875\n",
      "loss: 29.596364974975586\n",
      "loss: 35.08100509643555\n",
      "loss: 31.617414474487305\n",
      "loss: 33.81342315673828\n",
      "loss: 32.62236404418945\n",
      "loss: 32.42394256591797\n",
      "loss: 34.96544647216797\n",
      "loss: 33.43598556518555\n",
      "loss: 32.66536331176758\n",
      "loss: 32.45121765136719\n",
      "loss: 31.889366149902344\n",
      "loss: 31.597537994384766\n",
      "loss: 34.31800842285156\n",
      "loss: 31.61011505126953\n",
      "loss: 30.945804595947266\n",
      "loss: 29.108070373535156\n",
      "loss: 28.733163833618164\n",
      "loss: 31.32552146911621\n",
      "loss: 32.16658020019531\n",
      "loss: 32.48728942871094\n",
      "loss: 32.26038360595703\n",
      "loss: 30.75749397277832\n",
      "loss: 30.19123649597168\n",
      "loss: 32.26413345336914\n",
      "loss: 30.15434455871582\n",
      "loss: 31.287975311279297\n",
      "loss: 31.39324951171875\n",
      "loss: 29.336671829223633\n",
      "loss: 30.85277557373047\n",
      "loss: 30.81932830810547\n",
      "loss: 32.13243865966797\n",
      "loss: 30.940942764282227\n",
      "loss: 32.45815658569336\n",
      "loss: 32.596153259277344\n",
      "loss: 31.436172485351562\n",
      "loss: 30.791297912597656\n",
      "loss: 30.206966400146484\n",
      "loss: 28.28355598449707\n",
      "loss: 29.69903564453125\n",
      "loss: 31.53164291381836\n",
      "loss: 30.314125061035156\n",
      "loss: 31.338516235351562\n",
      "loss: 29.820337295532227\n",
      "loss: 32.298011779785156\n",
      "loss: 30.07191276550293\n",
      "loss: 30.461538314819336\n",
      "loss: 31.35671043395996\n",
      "loss: 29.336658477783203\n",
      "loss: 30.393203735351562\n",
      "loss: 31.22616958618164\n",
      "loss: 31.611068725585938\n",
      "loss: 29.863698959350586\n",
      "loss: 29.39008140563965\n",
      "loss: 29.336938858032227\n",
      "loss: 32.410438537597656\n",
      "loss: 30.776165008544922\n",
      "loss: 30.532461166381836\n",
      "loss: 30.31467628479004\n",
      "loss: 30.079225540161133\n",
      "loss: 29.845190048217773\n",
      "loss: 31.6004695892334\n",
      "loss: 30.586576461791992\n",
      "loss: 28.72281837463379\n",
      "loss: 29.146148681640625\n",
      "loss: 27.273698806762695\n",
      "loss: 28.52198600769043\n",
      "loss: 31.1450138092041\n",
      "loss: 30.398162841796875\n",
      "loss: 29.31592559814453\n",
      "loss: 28.912723541259766\n",
      "loss: 33.77617263793945\n",
      "loss: 30.461973190307617\n",
      "loss: 28.44205665588379\n",
      "loss: 27.532371520996094\n",
      "loss: 29.9027042388916\n",
      "loss: 29.874004364013672\n",
      "loss: 29.1274471282959\n",
      "loss: 28.1776123046875\n",
      "loss: 29.50486183166504\n",
      "loss: 29.827878952026367\n",
      "loss: 29.8819522857666\n",
      "loss: 29.979528427124023\n",
      "loss: 27.976795196533203\n",
      "loss: 29.31105613708496\n",
      "loss: 26.169187545776367\n",
      "loss: 28.231294631958008\n",
      "loss: 28.661312103271484\n",
      "loss: 29.564842224121094\n",
      "loss: 29.148630142211914\n",
      "loss: 30.48282814025879\n",
      "loss: 28.183490753173828\n",
      "loss: 28.8978328704834\n",
      "loss: 31.00847053527832\n",
      "loss: 27.513259887695312\n",
      "loss: 28.79707145690918\n",
      "loss: 28.827259063720703\n",
      "loss: 28.773876190185547\n",
      "loss: 29.154207229614258\n",
      "loss: 29.674407958984375\n",
      "loss: 27.897045135498047\n",
      "loss: 29.643083572387695\n",
      "loss: 27.786535263061523\n",
      "loss: 30.396240234375\n",
      "loss: 29.493776321411133\n",
      "loss: 26.878538131713867\n",
      "loss: 27.498727798461914\n",
      "loss: 28.410381317138672\n",
      "loss: 27.25914764404297\n",
      "loss: 30.319557189941406\n",
      "loss: 27.76319122314453\n",
      "loss: 27.13373565673828\n",
      "loss: 28.511648178100586\n",
      "loss: 26.161447525024414\n",
      "loss: 25.08855438232422\n",
      "loss: 27.824491500854492\n",
      "loss: 27.953046798706055\n",
      "loss: 27.4410457611084\n",
      "loss: 28.67477798461914\n",
      "loss: 24.89349365234375\n",
      "loss: 26.517200469970703\n",
      "loss: 28.563203811645508\n",
      "loss: 26.962383270263672\n",
      "loss: 27.388683319091797\n",
      "loss: 26.780393600463867\n",
      "loss: 26.727439880371094\n",
      "loss: 27.772762298583984\n",
      "loss: 26.99397087097168\n",
      "loss: 28.707868576049805\n",
      "loss: 26.582721710205078\n",
      "loss: 29.88637924194336\n",
      "loss: 25.599069595336914\n",
      "loss: 31.551315307617188\n",
      "loss: 25.952537536621094\n",
      "loss: 25.599008560180664\n",
      "loss: 26.69590187072754\n",
      "loss: 27.04535675048828\n",
      "loss: 26.44996452331543\n",
      "loss: 26.09625816345215\n",
      "loss: 25.760923385620117\n",
      "loss: 25.759845733642578\n",
      "loss: 26.32222557067871\n",
      "loss: 28.2578125\n",
      "loss: 25.534690856933594\n",
      "loss: 25.811784744262695\n",
      "loss: 25.929035186767578\n",
      "loss: 26.014324188232422\n",
      "loss: 28.178133010864258\n",
      "loss: 27.758060455322266\n",
      "loss: 26.003826141357422\n",
      "loss: 26.921091079711914\n",
      "loss: 27.401395797729492\n",
      "loss: 28.40896224975586\n",
      "loss: 26.013938903808594\n",
      "loss: 28.081457138061523\n",
      "loss: 26.728740692138672\n",
      "loss: 27.446748733520508\n",
      "loss: 27.442350387573242\n",
      "loss: 29.014223098754883\n",
      "loss: 26.999113082885742\n",
      "loss: 25.653654098510742\n",
      "loss: 25.6308650970459\n",
      "loss: 25.146928787231445\n",
      "loss: 24.764270782470703\n",
      "loss: 27.791519165039062\n",
      "loss: 27.139368057250977\n",
      "loss: 27.046308517456055\n",
      "loss: 26.758359909057617\n",
      "loss: 26.40817642211914\n",
      "loss: 27.760652542114258\n",
      "loss: 25.445201873779297\n",
      "loss: 24.869338989257812\n",
      "loss: 23.955364227294922\n",
      "loss: 25.661935806274414\n",
      "loss: 27.260574340820312\n",
      "loss: 25.563621520996094\n",
      "loss: 25.944965362548828\n",
      "loss: 26.099435806274414\n",
      "loss: 24.158334732055664\n",
      "loss: 27.590557098388672\n",
      "loss: 27.66901969909668\n",
      "loss: 24.742748260498047\n",
      "loss: 26.4180908203125\n",
      "loss: 22.618633270263672\n",
      "loss: 25.8881778717041\n",
      "loss: 25.711280822753906\n",
      "loss: 25.2017822265625\n",
      "loss: 24.232315063476562\n",
      "loss: 25.89719581604004\n",
      "loss: 24.991811752319336\n",
      "loss: 24.608060836791992\n",
      "loss: 24.6097354888916\n",
      "loss: 26.414505004882812\n",
      "loss: 25.96599006652832\n",
      "loss: 27.581937789916992\n",
      "loss: 26.89019203186035\n",
      "loss: 25.263519287109375\n",
      "loss: 27.37574577331543\n",
      "loss: 26.292879104614258\n",
      "loss: 26.550155639648438\n",
      "loss: 26.82903289794922\n",
      "loss: 26.050241470336914\n",
      "loss: 24.151578903198242\n",
      "loss: 26.572505950927734\n",
      "loss: 23.97013282775879\n",
      "loss: 25.057218551635742\n",
      "loss: 25.93949317932129\n",
      "loss: 24.019935607910156\n",
      "loss: 24.244510650634766\n",
      "loss: 25.76024627685547\n",
      "loss: 25.542728424072266\n",
      "loss: 24.74382209777832\n",
      "loss: 24.261693954467773\n",
      "loss: 26.920642852783203\n",
      "loss: 25.345733642578125\n",
      "loss: 21.44318389892578\n",
      "loss: 23.76643180847168\n",
      "loss: 25.86517906188965\n",
      "loss: 25.86870765686035\n",
      "loss: 22.587554931640625\n",
      "loss: 23.986385345458984\n",
      "loss: 24.267623901367188\n",
      "loss: 26.057716369628906\n",
      "loss: 23.22616195678711\n",
      "loss: 24.0651912689209\n",
      "loss: 23.671398162841797\n",
      "loss: 25.14197540283203\n",
      "loss: 23.162322998046875\n",
      "loss: 23.1973934173584\n",
      "loss: 23.937198638916016\n",
      "loss: 26.672975540161133\n",
      "loss: 22.82078742980957\n",
      "loss: 22.97760581970215\n",
      "loss: 21.626283645629883\n",
      "loss: 24.01421356201172\n",
      "loss: 26.01590347290039\n",
      "loss: 23.33260726928711\n",
      "loss: 23.56338119506836\n",
      "loss: 22.221593856811523\n",
      "loss: 20.10601043701172\n",
      "loss: 23.90913200378418\n",
      "loss: 21.611351013183594\n",
      "loss: 25.005151748657227\n",
      "loss: 23.378461837768555\n",
      "loss: 23.583276748657227\n",
      "loss: 24.30733299255371\n",
      "loss: 24.192087173461914\n",
      "loss: 21.00312614440918\n",
      "loss: 24.215686798095703\n",
      "loss: 25.125083923339844\n",
      "loss: 25.150728225708008\n",
      "loss: 24.038843154907227\n",
      "loss: 23.69883155822754\n",
      "loss: 21.161724090576172\n",
      "loss: 23.0859432220459\n",
      "loss: 23.862262725830078\n",
      "loss: 22.19123077392578\n",
      "loss: 22.412338256835938\n",
      "loss: 22.94420623779297\n",
      "loss: 21.83570098876953\n",
      "loss: 22.4714412689209\n",
      "loss: 23.114049911499023\n",
      "loss: 23.940717697143555\n",
      "loss: 24.805200576782227\n",
      "loss: 25.291362762451172\n",
      "loss: 23.23488426208496\n",
      "loss: 21.738447189331055\n",
      "loss: 22.983386993408203\n",
      "loss: 22.679853439331055\n",
      "loss: 21.72622299194336\n",
      "loss: 21.730777740478516\n",
      "loss: 22.72411346435547\n",
      "loss: 21.941619873046875\n",
      "loss: 19.616147994995117\n",
      "loss: 21.181476593017578\n",
      "loss: 23.297008514404297\n",
      "loss: 26.028841018676758\n",
      "loss: 21.996610641479492\n",
      "loss: 21.710935592651367\n",
      "loss: 21.369829177856445\n",
      "loss: 21.247652053833008\n",
      "loss: 22.63906478881836\n",
      "loss: 21.251962661743164\n",
      "loss: 21.67423439025879\n",
      "loss: 21.85171890258789\n",
      "loss: 21.928964614868164\n",
      "loss: 22.54349708557129\n",
      "loss: 21.328187942504883\n",
      "loss: 21.234193801879883\n",
      "loss: 22.005481719970703\n",
      "loss: 21.505979537963867\n",
      "loss: 20.89337921142578\n",
      "loss: 22.771181106567383\n",
      "loss: 20.545522689819336\n",
      "loss: 23.795194625854492\n",
      "loss: 22.42860984802246\n",
      "loss: 21.762956619262695\n",
      "loss: 21.023000717163086\n",
      "loss: 20.89807891845703\n",
      "loss: 20.888164520263672\n",
      "loss: 22.22650146484375\n",
      "loss: 22.388635635375977\n",
      "loss: 20.45992660522461\n",
      "loss: 23.28527069091797\n",
      "loss: 21.337560653686523\n",
      "loss: 23.212295532226562\n",
      "loss: 22.027029037475586\n",
      "loss: 23.450641632080078\n",
      "loss: 20.12201499938965\n",
      "loss: 20.795217514038086\n",
      "loss: 21.657459259033203\n",
      "loss: 21.889728546142578\n",
      "loss: 21.724023818969727\n",
      "loss: 22.087421417236328\n",
      "loss: 20.974689483642578\n",
      "loss: 23.564289093017578\n",
      "loss: 22.16390037536621\n",
      "loss: 21.40129280090332\n",
      "loss: 21.65930938720703\n",
      "loss: 21.13689422607422\n",
      "loss: 20.98345184326172\n",
      "loss: 21.828914642333984\n",
      "loss: 23.096099853515625\n",
      "loss: 21.827144622802734\n",
      "loss: 22.224708557128906\n",
      "loss: 21.25050926208496\n",
      "loss: 23.675992965698242\n",
      "loss: 21.287914276123047\n",
      "loss: 20.733211517333984\n",
      "loss: 22.05615997314453\n",
      "loss: 19.187549591064453\n",
      "loss: 20.929622650146484\n",
      "loss: 21.52474021911621\n",
      "loss: 22.05323028564453\n",
      "loss: 19.71280860900879\n",
      "loss: 23.5007266998291\n",
      "loss: 21.421199798583984\n",
      "loss: 20.04227066040039\n",
      "loss: 20.72423553466797\n",
      "loss: 22.668373107910156\n",
      "loss: 20.799121856689453\n",
      "loss: 20.897056579589844\n",
      "loss: 22.891010284423828\n",
      "loss: 19.165668487548828\n",
      "loss: 20.71220588684082\n",
      "loss: 19.640247344970703\n",
      "loss: 20.82084083557129\n",
      "loss: 21.63036346435547\n",
      "loss: 21.181554794311523\n",
      "loss: 20.478483200073242\n",
      "loss: 20.79277229309082\n",
      "loss: 23.173282623291016\n",
      "loss: 21.95209312438965\n",
      "loss: 20.683734893798828\n",
      "loss: 21.036359786987305\n",
      "loss: 21.054447174072266\n",
      "loss: 20.21798324584961\n",
      "loss: 20.988508224487305\n",
      "loss: 21.44365882873535\n",
      "loss: 17.935842514038086\n",
      "loss: 19.639978408813477\n",
      "loss: 21.590648651123047\n",
      "loss: 21.518091201782227\n",
      "loss: 24.091800689697266\n",
      "loss: 21.346843719482422\n",
      "loss: 19.893171310424805\n",
      "loss: 20.32270622253418\n",
      "loss: 21.432275772094727\n",
      "loss: 20.98891830444336\n",
      "loss: 19.977188110351562\n",
      "loss: 19.736482620239258\n",
      "loss: 22.245620727539062\n",
      "loss: 19.18316078186035\n",
      "loss: 22.306747436523438\n",
      "loss: 20.978939056396484\n",
      "loss: 20.537063598632812\n",
      "loss: 18.46443748474121\n",
      "loss: 20.77869987487793\n",
      "loss: 21.07048225402832\n",
      "loss: 21.930944442749023\n",
      "loss: 19.882631301879883\n",
      "loss: 21.39789581298828\n",
      "loss: 20.21393394470215\n",
      "loss: 21.950132369995117\n",
      "loss: 19.910259246826172\n",
      "loss: 20.884693145751953\n",
      "loss: 19.46505355834961\n",
      "loss: 21.282596588134766\n",
      "loss: 18.866613388061523\n",
      "loss: 19.255813598632812\n",
      "loss: 22.20164680480957\n",
      "loss: 20.275798797607422\n",
      "loss: 20.671384811401367\n",
      "loss: 20.650550842285156\n",
      "loss: 18.656021118164062\n",
      "loss: 21.24871063232422\n",
      "loss: 19.336292266845703\n",
      "loss: 19.467296600341797\n",
      "loss: 20.59973907470703\n",
      "loss: 17.96733856201172\n",
      "loss: 20.67506217956543\n",
      "loss: 19.60065460205078\n",
      "loss: 18.463560104370117\n",
      "loss: 19.619421005249023\n",
      "loss: 21.138696670532227\n",
      "loss: 19.008403778076172\n",
      "loss: 19.501413345336914\n",
      "loss: 19.30821990966797\n",
      "loss: 20.18818473815918\n",
      "loss: 19.477962493896484\n",
      "loss: 18.983665466308594\n",
      "loss: 18.916404724121094\n",
      "loss: 18.31293296813965\n",
      "loss: 19.021257400512695\n",
      "loss: 20.965124130249023\n",
      "loss: 17.382421493530273\n",
      "loss: 19.558448791503906\n",
      "loss: 20.523845672607422\n",
      "loss: 17.518598556518555\n",
      "loss: 19.665401458740234\n",
      "loss: 19.183950424194336\n",
      "loss: 15.742345809936523\n",
      "loss: 21.2978458404541\n",
      "loss: 20.03561782836914\n",
      "loss: 17.49827003479004\n",
      "loss: 16.869115829467773\n",
      "loss: 17.582260131835938\n",
      "loss: 16.87245750427246\n",
      "loss: 17.72123908996582\n",
      "loss: 20.60152244567871\n",
      "loss: 18.74095344543457\n",
      "loss: 16.259891510009766\n",
      "loss: 18.874591827392578\n",
      "loss: 18.06981086730957\n",
      "loss: 21.606103897094727\n",
      "loss: 18.232887268066406\n",
      "loss: 18.982524871826172\n",
      "loss: 17.881961822509766\n",
      "loss: 22.239097595214844\n",
      "loss: 18.474992752075195\n",
      "loss: 18.288082122802734\n",
      "loss: 16.890338897705078\n",
      "loss: 18.865991592407227\n",
      "loss: 17.791454315185547\n",
      "loss: 16.631122589111328\n",
      "loss: 16.67963409423828\n",
      "loss: 19.4957218170166\n",
      "loss: 17.39330291748047\n",
      "loss: 18.99627113342285\n",
      "loss: 19.385404586791992\n",
      "loss: 17.852102279663086\n",
      "loss: 18.520519256591797\n",
      "loss: 18.38164710998535\n",
      "loss: 17.109298706054688\n",
      "loss: 18.868186950683594\n",
      "loss: 17.909137725830078\n",
      "loss: 16.962305068969727\n",
      "loss: 17.5559024810791\n",
      "loss: 18.63487434387207\n",
      "loss: 17.69275665283203\n",
      "loss: 18.163570404052734\n",
      "loss: 15.944276809692383\n",
      "loss: 19.76494026184082\n",
      "loss: 18.818817138671875\n",
      "loss: 19.41625213623047\n",
      "loss: 17.967546463012695\n",
      "loss: 18.421234130859375\n",
      "loss: 18.083847045898438\n",
      "loss: 19.154211044311523\n",
      "loss: 18.393905639648438\n",
      "loss: 18.020912170410156\n",
      "loss: 19.303396224975586\n",
      "loss: 17.227121353149414\n",
      "loss: 18.358259201049805\n",
      "loss: 19.578304290771484\n",
      "loss: 18.874181747436523\n",
      "loss: 19.411819458007812\n",
      "loss: 17.643049240112305\n",
      "loss: 18.019773483276367\n",
      "loss: 18.066312789916992\n",
      "loss: 16.318340301513672\n",
      "loss: 17.55303955078125\n",
      "loss: 18.90456771850586\n",
      "loss: 16.800596237182617\n",
      "loss: 16.752511978149414\n",
      "loss: 18.50135040283203\n",
      "loss: 16.342683792114258\n",
      "loss: 16.6706600189209\n",
      "loss: 18.18980598449707\n",
      "loss: 17.756147384643555\n",
      "loss: 18.686559677124023\n",
      "loss: 16.978046417236328\n",
      "loss: 17.365100860595703\n",
      "loss: 16.86180305480957\n",
      "loss: 17.23151397705078\n",
      "loss: 19.165998458862305\n",
      "loss: 16.013389587402344\n",
      "loss: 17.12278175354004\n",
      "loss: 16.071439743041992\n",
      "loss: 17.88587760925293\n",
      "loss: 16.882251739501953\n",
      "loss: 16.656471252441406\n",
      "loss: 17.68955421447754\n",
      "loss: 18.297889709472656\n",
      "loss: 15.765973091125488\n",
      "loss: 17.586950302124023\n",
      "loss: 17.008153915405273\n",
      "loss: 19.181095123291016\n",
      "loss: 16.550485610961914\n",
      "loss: 17.07931137084961\n",
      "loss: 15.961458206176758\n",
      "loss: 16.018962860107422\n",
      "loss: 17.272390365600586\n",
      "loss: 16.37297248840332\n",
      "loss: 16.68309211730957\n",
      "loss: 16.983125686645508\n",
      "loss: 18.107561111450195\n",
      "loss: 16.698549270629883\n",
      "loss: 16.90799331665039\n",
      "loss: 17.393638610839844\n",
      "loss: 17.411998748779297\n",
      "loss: 16.17960548400879\n",
      "loss: 16.643836975097656\n",
      "loss: 16.38295555114746\n",
      "loss: 17.9794921875\n",
      "loss: 16.398420333862305\n",
      "loss: 16.78251838684082\n",
      "loss: 17.816749572753906\n",
      "loss: 16.925647735595703\n",
      "loss: 16.810388565063477\n",
      "loss: 16.900543212890625\n",
      "loss: 16.13730239868164\n",
      "loss: 16.06449317932129\n",
      "loss: 19.380966186523438\n",
      "loss: 16.16046714782715\n",
      "loss: 18.29364776611328\n",
      "loss: 15.507290840148926\n",
      "loss: 17.195478439331055\n",
      "loss: 17.216049194335938\n",
      "loss: 15.998364448547363\n",
      "loss: 17.129493713378906\n",
      "loss: 16.093002319335938\n",
      "loss: 16.410905838012695\n",
      "loss: 17.108238220214844\n",
      "loss: 16.46994972229004\n",
      "loss: 15.785274505615234\n",
      "loss: 14.9201021194458\n",
      "loss: 19.034486770629883\n",
      "loss: 17.54437255859375\n",
      "loss: 16.86720085144043\n",
      "loss: 16.639204025268555\n",
      "loss: 16.980365753173828\n",
      "loss: 15.213713645935059\n",
      "loss: 15.522941589355469\n",
      "loss: 16.29029083251953\n",
      "loss: 16.602859497070312\n",
      "loss: 16.342618942260742\n",
      "loss: 15.373862266540527\n",
      "loss: 18.576217651367188\n",
      "loss: 15.572442054748535\n",
      "loss: 15.53821086883545\n",
      "loss: 15.82315731048584\n",
      "loss: 15.859274864196777\n",
      "loss: 16.46286392211914\n",
      "loss: 15.003151893615723\n",
      "loss: 15.873371124267578\n",
      "loss: 16.37386703491211\n",
      "loss: 16.5115966796875\n",
      "loss: 16.536216735839844\n",
      "loss: 16.109769821166992\n",
      "loss: 16.067262649536133\n",
      "loss: 15.078431129455566\n",
      "loss: 15.875646591186523\n",
      "loss: 14.208160400390625\n",
      "loss: 14.954005241394043\n",
      "loss: 16.713153839111328\n",
      "loss: 16.274398803710938\n",
      "loss: 17.143930435180664\n",
      "loss: 16.173866271972656\n",
      "loss: 16.382831573486328\n",
      "loss: 15.483370780944824\n",
      "loss: 15.179853439331055\n",
      "loss: 15.788897514343262\n",
      "loss: 15.880769729614258\n",
      "loss: 15.897189140319824\n",
      "loss: 13.419559478759766\n",
      "loss: 15.872467041015625\n",
      "loss: 15.261466026306152\n",
      "loss: 16.4489803314209\n",
      "loss: 15.722867012023926\n",
      "loss: 16.03433609008789\n",
      "loss: 17.34459114074707\n",
      "loss: 17.578691482543945\n",
      "loss: 16.776752471923828\n",
      "loss: 17.51936149597168\n",
      "loss: 15.828104019165039\n",
      "loss: 15.977209091186523\n",
      "loss: 13.506914138793945\n",
      "loss: 15.86497688293457\n",
      "loss: 16.314191818237305\n",
      "loss: 17.0971622467041\n",
      "loss: 17.8714656829834\n",
      "loss: 16.436983108520508\n",
      "loss: 16.309629440307617\n",
      "loss: 17.894210815429688\n",
      "loss: 14.177839279174805\n",
      "loss: 13.397850036621094\n",
      "loss: 15.15302562713623\n",
      "loss: 16.143672943115234\n",
      "loss: 13.724056243896484\n",
      "loss: 18.154312133789062\n",
      "loss: 15.876509666442871\n",
      "loss: 16.510692596435547\n",
      "loss: 16.681535720825195\n",
      "loss: 16.148326873779297\n",
      "loss: 14.904348373413086\n",
      "loss: 15.248703002929688\n",
      "loss: 15.31972599029541\n",
      "loss: 16.91518211364746\n",
      "loss: 14.81604290008545\n",
      "loss: 14.946619987487793\n",
      "loss: 15.455550193786621\n",
      "loss: 15.794821739196777\n",
      "loss: 14.075328826904297\n",
      "loss: 15.474769592285156\n",
      "loss: 14.631240844726562\n",
      "loss: 16.402175903320312\n",
      "loss: 14.648959159851074\n",
      "loss: 16.092876434326172\n",
      "loss: 16.356477737426758\n",
      "loss: 16.428972244262695\n",
      "loss: 14.228139877319336\n",
      "loss: 14.076250076293945\n",
      "loss: 16.785737991333008\n",
      "loss: 16.379941940307617\n",
      "loss: 15.283110618591309\n",
      "loss: 15.717265129089355\n",
      "loss: 15.63647747039795\n",
      "loss: 16.092830657958984\n",
      "loss: 14.336193084716797\n",
      "loss: 14.909887313842773\n",
      "loss: 14.07530403137207\n",
      "loss: 14.984203338623047\n",
      "loss: 15.406432151794434\n",
      "loss: 14.3932466506958\n",
      "loss: 14.65953540802002\n",
      "loss: 16.363325119018555\n",
      "loss: 13.381179809570312\n",
      "loss: 17.006376266479492\n",
      "loss: 14.476000785827637\n",
      "loss: 13.141541481018066\n",
      "loss: 16.143386840820312\n",
      "loss: 16.193981170654297\n",
      "loss: 13.693711280822754\n",
      "loss: 14.555822372436523\n",
      "loss: 15.898688316345215\n",
      "loss: 14.168415069580078\n",
      "loss: 15.003053665161133\n",
      "loss: 14.054875373840332\n",
      "loss: 12.924162864685059\n",
      "loss: 14.146275520324707\n",
      "loss: 14.517523765563965\n",
      "loss: 15.492496490478516\n",
      "loss: 13.930023193359375\n",
      "loss: 12.94719409942627\n",
      "loss: 15.837318420410156\n",
      "loss: 14.09197998046875\n",
      "loss: 14.99148941040039\n",
      "loss: 14.341193199157715\n",
      "loss: 14.328584671020508\n",
      "loss: 14.5975980758667\n",
      "loss: 14.99832534790039\n",
      "loss: 13.243448257446289\n",
      "loss: 13.933197021484375\n",
      "loss: 14.245427131652832\n",
      "loss: 14.016366958618164\n",
      "loss: 14.651530265808105\n",
      "loss: 14.8472261428833\n",
      "loss: 14.350728034973145\n",
      "loss: 13.252946853637695\n",
      "loss: 14.232619285583496\n",
      "loss: 14.20186996459961\n",
      "loss: 14.887016296386719\n",
      "loss: 15.137300491333008\n",
      "loss: 12.570335388183594\n",
      "loss: 15.218615531921387\n",
      "loss: 14.626091003417969\n",
      "loss: 13.786092758178711\n",
      "loss: 15.314130783081055\n",
      "loss: 14.020486831665039\n",
      "loss: 15.401601791381836\n",
      "loss: 12.808728218078613\n",
      "loss: 13.57199478149414\n",
      "loss: 16.314542770385742\n",
      "loss: 14.302002906799316\n",
      "loss: 13.257524490356445\n",
      "loss: 14.736473083496094\n",
      "loss: 13.05671501159668\n",
      "loss: 12.547688484191895\n",
      "loss: 14.530294418334961\n",
      "loss: 15.102815628051758\n",
      "loss: 14.585986137390137\n",
      "loss: 12.442407608032227\n",
      "loss: 13.998546600341797\n",
      "loss: 13.84394359588623\n",
      "loss: 14.704070091247559\n",
      "loss: 13.55368423461914\n",
      "loss: 14.829610824584961\n",
      "loss: 14.774608612060547\n",
      "loss: 13.22418212890625\n",
      "loss: 14.780170440673828\n",
      "loss: 13.520638465881348\n",
      "loss: 13.395241737365723\n",
      "loss: 11.167884826660156\n",
      "loss: 13.9247465133667\n",
      "loss: 12.209747314453125\n",
      "loss: 13.763758659362793\n",
      "loss: 13.64607048034668\n",
      "loss: 11.737747192382812\n",
      "loss: 14.853290557861328\n",
      "loss: 14.694195747375488\n",
      "loss: 12.677905082702637\n",
      "loss: 13.508959770202637\n",
      "loss: 12.650090217590332\n",
      "loss: 13.336758613586426\n",
      "loss: 12.802830696105957\n",
      "loss: 13.063861846923828\n",
      "loss: 13.585929870605469\n",
      "loss: 15.361480712890625\n",
      "loss: 12.557059288024902\n",
      "loss: 13.148987770080566\n",
      "loss: 13.807127952575684\n",
      "loss: 13.019696235656738\n",
      "loss: 13.013395309448242\n",
      "loss: 13.554332733154297\n",
      "loss: 13.846924781799316\n",
      "loss: 15.272491455078125\n",
      "loss: 13.74101734161377\n",
      "loss: 12.625839233398438\n",
      "loss: 13.090197563171387\n",
      "loss: 14.111480712890625\n",
      "loss: 13.47672176361084\n",
      "loss: 13.188119888305664\n",
      "loss: 12.60367488861084\n",
      "loss: 13.366692543029785\n",
      "loss: 15.375926971435547\n",
      "loss: 13.411940574645996\n",
      "loss: 11.71087646484375\n",
      "loss: 13.647499084472656\n",
      "loss: 13.259172439575195\n",
      "loss: 13.840103149414062\n",
      "loss: 14.269832611083984\n",
      "loss: 15.524690628051758\n",
      "loss: 13.467550277709961\n",
      "loss: 13.25609016418457\n",
      "loss: 14.289331436157227\n",
      "loss: 14.910074234008789\n",
      "loss: 12.91119384765625\n",
      "loss: 12.243326187133789\n",
      "loss: 12.599708557128906\n",
      "loss: 11.72839069366455\n",
      "loss: 12.492277145385742\n",
      "loss: 13.283327102661133\n",
      "loss: 11.969420433044434\n",
      "loss: 13.904571533203125\n",
      "loss: 12.281964302062988\n",
      "loss: 13.426419258117676\n",
      "loss: 13.31254768371582\n",
      "loss: 13.021621704101562\n",
      "loss: 14.580252647399902\n",
      "loss: 13.684814453125\n",
      "loss: 13.498836517333984\n",
      "loss: 13.417925834655762\n",
      "loss: 12.059322357177734\n",
      "loss: 11.016879081726074\n",
      "loss: 11.711873054504395\n",
      "loss: 12.856215476989746\n",
      "loss: 12.228899002075195\n",
      "loss: 11.91935920715332\n",
      "loss: 11.593192100524902\n",
      "loss: 12.236693382263184\n",
      "loss: 12.810384750366211\n",
      "loss: 12.143844604492188\n",
      "loss: 12.418051719665527\n",
      "loss: 12.913862228393555\n",
      "loss: 14.942095756530762\n",
      "loss: 12.246238708496094\n",
      "loss: 13.51392650604248\n",
      "loss: 10.802897453308105\n",
      "loss: 11.775099754333496\n",
      "loss: 10.970307350158691\n",
      "loss: 10.97214412689209\n",
      "loss: 10.504695892333984\n",
      "loss: 12.798280715942383\n",
      "loss: 12.322731971740723\n",
      "loss: 12.778618812561035\n",
      "loss: 11.577545166015625\n",
      "loss: 11.635544776916504\n",
      "loss: 13.045324325561523\n",
      "loss: 12.786227226257324\n",
      "loss: 13.829001426696777\n",
      "loss: 12.430747985839844\n",
      "loss: 13.285011291503906\n",
      "loss: 12.011151313781738\n",
      "loss: 11.988243103027344\n",
      "loss: 12.998089790344238\n",
      "loss: 10.515742301940918\n",
      "loss: 11.150774955749512\n",
      "loss: 11.582627296447754\n",
      "loss: 11.999835014343262\n",
      "loss: 11.873967170715332\n",
      "loss: 11.965629577636719\n",
      "loss: 12.072503089904785\n",
      "loss: 13.037878036499023\n",
      "loss: 11.376191139221191\n",
      "loss: 11.312765121459961\n",
      "loss: 12.789985656738281\n",
      "loss: 9.968117713928223\n",
      "loss: 13.945878982543945\n",
      "loss: 12.064718246459961\n",
      "loss: 13.716044425964355\n",
      "loss: 11.210227966308594\n",
      "loss: 12.659421920776367\n",
      "loss: 11.414621353149414\n",
      "loss: 10.955026626586914\n",
      "loss: 10.24129867553711\n",
      "loss: 14.521724700927734\n",
      "loss: 12.62441635131836\n",
      "loss: 12.859071731567383\n",
      "loss: 12.683083534240723\n",
      "loss: 11.79189395904541\n",
      "loss: 11.002338409423828\n",
      "loss: 12.946465492248535\n",
      "loss: 11.316411972045898\n",
      "loss: 11.975110054016113\n",
      "loss: 10.709519386291504\n",
      "loss: 11.398869514465332\n",
      "loss: 11.604104995727539\n",
      "loss: 11.982014656066895\n",
      "loss: 13.155428886413574\n",
      "loss: 11.746167182922363\n",
      "loss: 12.125410079956055\n",
      "loss: 11.856834411621094\n",
      "loss: 13.67786693572998\n",
      "loss: 12.028905868530273\n",
      "loss: 11.3892822265625\n",
      "loss: 12.260824203491211\n",
      "loss: 11.58085823059082\n",
      "loss: 11.567373275756836\n",
      "loss: 12.53080940246582\n",
      "loss: 11.194899559020996\n",
      "loss: 11.646441459655762\n",
      "loss: 11.34679889678955\n",
      "loss: 12.098029136657715\n",
      "loss: 13.052680969238281\n",
      "loss: 12.773703575134277\n",
      "loss: 10.749154090881348\n",
      "loss: 11.83332347869873\n",
      "loss: 10.128021240234375\n",
      "loss: 12.259568214416504\n",
      "loss: 10.257226943969727\n",
      "loss: 10.665925025939941\n",
      "loss: 11.015047073364258\n",
      "loss: 11.065163612365723\n",
      "loss: 9.879645347595215\n",
      "loss: 11.918893814086914\n",
      "loss: 13.44658374786377\n",
      "loss: 12.049764633178711\n",
      "loss: 11.205024719238281\n",
      "loss: 11.544774055480957\n",
      "loss: 11.826835632324219\n",
      "loss: 10.372072219848633\n",
      "loss: 11.818702697753906\n",
      "loss: 12.475403785705566\n",
      "loss: 11.414419174194336\n",
      "loss: 11.372076988220215\n",
      "loss: 11.022560119628906\n",
      "loss: 10.14031982421875\n",
      "loss: 12.541932106018066\n",
      "loss: 11.211191177368164\n",
      "loss: 11.246233940124512\n",
      "loss: 11.154160499572754\n",
      "loss: 11.81018352508545\n",
      "loss: 11.686751365661621\n",
      "loss: 12.137107849121094\n",
      "loss: 9.851460456848145\n",
      "loss: 12.820511817932129\n",
      "loss: 11.217994689941406\n",
      "loss: 11.128035545349121\n",
      "loss: 12.779603004455566\n",
      "loss: 11.384673118591309\n",
      "loss: 10.538207054138184\n",
      "loss: 12.696955680847168\n",
      "loss: 10.715475082397461\n",
      "loss: 11.442230224609375\n",
      "loss: 9.790105819702148\n",
      "loss: 12.6979398727417\n",
      "loss: 10.136319160461426\n",
      "loss: 12.667630195617676\n",
      "loss: 12.281599044799805\n",
      "loss: 11.330381393432617\n",
      "loss: 11.525956153869629\n",
      "loss: 11.555659294128418\n",
      "loss: 10.290804862976074\n",
      "loss: 11.398300170898438\n",
      "loss: 9.190637588500977\n",
      "loss: 11.35572624206543\n",
      "loss: 12.235390663146973\n",
      "loss: 12.152734756469727\n",
      "loss: 11.010430335998535\n",
      "loss: 11.15963363647461\n",
      "loss: 10.232378005981445\n",
      "loss: 10.633291244506836\n",
      "loss: 10.950286865234375\n",
      "loss: 10.996349334716797\n",
      "loss: 10.457193374633789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m hybrid_model(input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hybrid_model.train()\n",
    "optimizer = torch.optim.AdamW(params=trainer_params, lr=1e-5)\n",
    "for e in range(50):\n",
    "    # shuffle the dataset\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        outputs = hybrid_model(input_ids = batch['input_ids'].to(device), attention_mask = batch['attention_mask'].to(device), labels = batch['labels'].to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"test\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 18355/18355 [00:07<00:00, 2513.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=test_dataset.column_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test, batch_size=8, collate_fn=data_collator,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.6471586227417\n",
      "loss: 11.454437255859375\n",
      "loss: 12.421030044555664\n",
      "loss: 11.016474723815918\n",
      "loss: 11.603528022766113\n",
      "loss: 11.723335266113281\n",
      "loss: 9.749794006347656\n",
      "loss: 12.039562225341797\n",
      "loss: 11.02593994140625\n",
      "loss: 10.021510124206543\n",
      "loss: 11.257247924804688\n",
      "loss: 12.059006690979004\n",
      "loss: 9.701502799987793\n",
      "loss: 11.042990684509277\n",
      "loss: 10.825369834899902\n",
      "loss: 9.80688762664795\n",
      "loss: 11.146143913269043\n",
      "loss: 10.33370590209961\n",
      "loss: 10.876644134521484\n",
      "loss: 10.777077674865723\n",
      "loss: 10.729930877685547\n",
      "loss: 10.406472206115723\n",
      "loss: 10.843860626220703\n",
      "loss: 11.41571044921875\n",
      "loss: 10.354085922241211\n",
      "loss: 11.644628524780273\n",
      "loss: 11.275474548339844\n",
      "loss: 9.680427551269531\n",
      "loss: 10.180423736572266\n",
      "loss: 12.415082931518555\n",
      "loss: 10.509671211242676\n",
      "loss: 10.893714904785156\n",
      "loss: 10.448331832885742\n",
      "loss: 10.659329414367676\n",
      "loss: 10.888725280761719\n",
      "loss: 10.161733627319336\n",
      "loss: 10.17746639251709\n",
      "loss: 9.743611335754395\n",
      "loss: 11.232806205749512\n",
      "loss: 10.32297134399414\n",
      "loss: 11.415425300598145\n",
      "loss: 9.60546875\n",
      "loss: 11.26065444946289\n",
      "loss: 11.526122093200684\n",
      "loss: 9.389876365661621\n",
      "loss: 10.527924537658691\n",
      "loss: 9.568074226379395\n",
      "loss: 9.333849906921387\n",
      "loss: 12.552271842956543\n",
      "loss: 10.471702575683594\n",
      "loss: 10.150224685668945\n",
      "loss: 11.639690399169922\n",
      "loss: 13.01574993133545\n",
      "loss: 13.123174667358398\n",
      "loss: 10.587646484375\n",
      "loss: 11.799446105957031\n",
      "loss: 11.76740550994873\n",
      "loss: 13.141104698181152\n",
      "loss: 11.334461212158203\n",
      "loss: 10.42239761352539\n",
      "loss: 11.558030128479004\n",
      "loss: 12.404213905334473\n",
      "loss: 11.961906433105469\n",
      "loss: 10.315513610839844\n",
      "loss: 10.210184097290039\n",
      "loss: 9.921031951904297\n",
      "loss: 11.203271865844727\n",
      "loss: 10.630537986755371\n",
      "loss: 10.281503677368164\n",
      "loss: 9.99941635131836\n",
      "loss: 10.958935737609863\n",
      "loss: 12.016792297363281\n",
      "loss: 11.589210510253906\n",
      "loss: 10.201626777648926\n",
      "loss: 12.995495796203613\n",
      "loss: 10.647774696350098\n",
      "loss: 10.021916389465332\n",
      "loss: 12.902934074401855\n",
      "loss: 10.583770751953125\n",
      "loss: 11.247152328491211\n",
      "loss: 11.794303894042969\n",
      "loss: 12.205304145812988\n",
      "loss: 11.241484642028809\n",
      "loss: 10.010477066040039\n",
      "loss: 10.366453170776367\n",
      "loss: 11.70109748840332\n",
      "loss: 11.519468307495117\n",
      "loss: 12.545806884765625\n",
      "loss: 11.304708480834961\n",
      "loss: 11.596668243408203\n",
      "loss: 10.114392280578613\n",
      "loss: 12.40833854675293\n",
      "loss: 10.533294677734375\n",
      "loss: 11.575797080993652\n",
      "loss: 9.221443176269531\n",
      "loss: 11.491958618164062\n",
      "loss: 9.42169189453125\n",
      "loss: 10.586161613464355\n",
      "loss: 10.158797264099121\n",
      "loss: 11.716859817504883\n",
      "loss: 9.072969436645508\n",
      "loss: 9.50437068939209\n",
      "loss: 9.972051620483398\n",
      "loss: 10.355175018310547\n",
      "loss: 10.370401382446289\n",
      "loss: 10.380753517150879\n",
      "loss: 9.470669746398926\n",
      "loss: 10.295466423034668\n",
      "loss: 10.08307933807373\n",
      "loss: 9.805737495422363\n",
      "loss: 11.10330581665039\n",
      "loss: 10.475053787231445\n",
      "loss: 11.774857521057129\n",
      "loss: 10.267841339111328\n",
      "loss: 11.996015548706055\n",
      "loss: 10.480685234069824\n",
      "loss: 11.743173599243164\n",
      "loss: 11.496025085449219\n",
      "loss: 11.223490715026855\n",
      "loss: 9.322819709777832\n",
      "loss: 12.6919584274292\n",
      "loss: 11.43216609954834\n",
      "loss: 11.341547966003418\n",
      "loss: 10.286116600036621\n",
      "loss: 11.277754783630371\n",
      "loss: 11.876443862915039\n",
      "loss: 11.072050094604492\n",
      "loss: 8.71791934967041\n",
      "loss: 11.320647239685059\n",
      "loss: 9.538275718688965\n",
      "loss: 11.842233657836914\n",
      "loss: 11.127564430236816\n",
      "loss: 12.606203079223633\n",
      "loss: 12.004766464233398\n",
      "loss: 11.449260711669922\n",
      "loss: 10.346379280090332\n",
      "loss: 12.328536033630371\n",
      "loss: 12.599676132202148\n",
      "loss: 11.013854026794434\n",
      "loss: 10.805184364318848\n",
      "loss: 11.723048210144043\n",
      "loss: 10.78571605682373\n",
      "loss: 11.81136417388916\n",
      "loss: 11.758543968200684\n",
      "loss: 9.637946128845215\n",
      "loss: 11.761934280395508\n",
      "loss: 10.76142406463623\n",
      "loss: 10.56767463684082\n",
      "loss: 10.119718551635742\n",
      "loss: 11.969366073608398\n",
      "loss: 12.72424602508545\n",
      "loss: 11.313363075256348\n",
      "loss: 10.775402069091797\n",
      "loss: 10.635321617126465\n",
      "loss: 11.754463195800781\n",
      "loss: 10.360295295715332\n",
      "loss: 10.79595947265625\n",
      "loss: 11.895615577697754\n",
      "loss: 11.203656196594238\n",
      "loss: 12.8357515335083\n",
      "loss: 12.078536033630371\n",
      "loss: 10.677376747131348\n",
      "loss: 12.31266975402832\n",
      "loss: 10.830887794494629\n",
      "loss: 11.111573219299316\n",
      "loss: 11.802838325500488\n",
      "loss: 11.840530395507812\n",
      "loss: 11.304669380187988\n",
      "loss: 10.650491714477539\n",
      "loss: 11.75762939453125\n",
      "loss: 10.040857315063477\n",
      "loss: 10.03577995300293\n",
      "loss: 11.00564956665039\n",
      "loss: 11.500540733337402\n",
      "loss: 10.611698150634766\n",
      "loss: 10.040006637573242\n",
      "loss: 11.164319038391113\n",
      "loss: 10.395743370056152\n",
      "loss: 10.662558555603027\n",
      "loss: 10.012260437011719\n",
      "loss: 10.277043342590332\n",
      "loss: 10.682229995727539\n",
      "loss: 10.342377662658691\n",
      "loss: 10.39005184173584\n",
      "loss: 9.862449645996094\n",
      "loss: 10.257964134216309\n",
      "loss: 9.732556343078613\n",
      "loss: 10.497052192687988\n",
      "loss: 10.494950294494629\n",
      "loss: 9.640305519104004\n",
      "loss: 10.643200874328613\n",
      "loss: 11.40652084350586\n",
      "loss: 11.49809455871582\n",
      "loss: 9.770681381225586\n",
      "loss: 11.391945838928223\n",
      "loss: 9.181967735290527\n",
      "loss: 10.610703468322754\n",
      "loss: 11.102782249450684\n",
      "loss: 9.694428443908691\n",
      "loss: 10.524036407470703\n",
      "loss: 11.145686149597168\n",
      "loss: 10.946916580200195\n",
      "loss: 10.802847862243652\n",
      "loss: 11.322425842285156\n",
      "loss: 12.197094917297363\n",
      "loss: 12.232553482055664\n",
      "loss: 10.884647369384766\n",
      "loss: 10.969947814941406\n",
      "loss: 11.93154239654541\n",
      "loss: 11.719980239868164\n",
      "loss: 11.222864151000977\n",
      "loss: 11.61728572845459\n",
      "loss: 10.103730201721191\n",
      "loss: 11.165421485900879\n",
      "loss: 9.105833053588867\n",
      "loss: 10.769305229187012\n",
      "loss: 11.191469192504883\n",
      "loss: 10.7042236328125\n",
      "loss: 10.368208885192871\n",
      "loss: 10.185190200805664\n",
      "loss: 10.855640411376953\n",
      "loss: 9.826762199401855\n",
      "loss: 10.101152420043945\n",
      "loss: 9.82785415649414\n",
      "loss: 10.749361991882324\n",
      "loss: 9.327011108398438\n",
      "loss: 10.309907913208008\n",
      "loss: 10.642939567565918\n",
      "loss: 11.801623344421387\n",
      "loss: 11.596275329589844\n"
     ]
    }
   ],
   "source": [
    "hybrid_model.eval()\n",
    "with torch.no_grad():\n",
    "    # shuffle the dataset\n",
    "    for i, batch in enumerate(testloader):\n",
    "        #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        outputs = hybrid_model(input_ids = batch['input_ids'].to(device), attention_mask = batch['attention_mask'].to(device), labels = batch['labels'].to(device))\n",
    "        loss = outputs.loss\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer_params, \"projector_eli5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hybrid_model.state_dict(), \"hybrid_model_eli5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_eli5[0][\"input_ids\"]))\n",
    "print(len(tokenized_eli5[-1][\"input_ids\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "hybrid_model = HybridModel(transformer_model=transformer_model.transformer, mamba_model=mamba_model.backbone,proj_type= \"gressf\", n_hybrid_blocks=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_loaded  = torch.load(\"./hybrid_model_eli5.pth\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model.load_state_dict(state_dict_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0a62366f6242dba5aee42fd6889719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed36b3b41841465eacec6f4c65024c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/fahad-touseef/manticore-hybrid-gptneo-mamba/commit/24480fc634e02953777895713fccc325753549c6', commit_message='Push model using huggingface_hub.', commit_description='', oid='24480fc634e02953777895713fccc325753549c6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/fahad-touseef/manticore-hybrid-gptneo-mamba', endpoint='https://huggingface.co', repo_type='model', repo_id='fahad-touseef/manticore-hybrid-gptneo-mamba'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model.push_to_hub(\"fahad-touseef/manticore-hybrid-gptneo-mamba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# shuffle the dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m#batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "hybrid_model.eval()\n",
    "with torch.no_grad():\n",
    "    # shuffle the dataset\n",
    "    for i, batch in enumerate(testloader):\n",
    "        #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        outputs = hybrid_model(input_ids = batch['input_ids'].to(device), attention_mask = batch['attention_mask'].to(device), labels = batch['labels'].to(device))\n",
    "        loss = outputs.loss\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return transformer_tokenizer(examples[\"text\"],truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'])\n",
    "# Create the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=transformer_tokenizer, mlm=False)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=8, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1532,   345,   467,  7374,   736, 41291,    11,  6729,  1477,  2577,\n",
      "          278,    11,   393,  3817,  5718,  1586,    11,   345,   389,  1016,\n",
      "          284,   765,   284,  2222,   379,  1551,   530,  5166,   286,   285,\n",
      "        34978,    13,  2773,   661,   481,   772,  2222,   734, 14729,   287,\n",
      "         1339,   530,  3011,  9583,    13, 31342,    11,   314,  2222,   257,\n",
      "         5166,   286,   285, 34978,   329, 23125,   290,   257,  5166,   286,\n",
      "        18051,   329,  2087, 50003,    13,   198,  2215, 17246,   285, 34978,\n",
      "          290, 18051,   329,  7374,  3403,    11,   994,   389,   257,  3155,\n",
      "          286,  1276,    12, 14150,  3033,   326,   345,   815,   804,   329,\n",
      "           25,   198,  8413, 21985,  9493,   364,    25,   921,   761,   257,\n",
      "          285,  2621,   393, 29144,  1080,   326,   468,  9493,   364,   290,\n",
      "          257, 35973,  7582,   523,   345,   836,   447,   247,    83,   625,\n",
      "        25080,   290, 15488,    13, 19372,   265,   287,  7374,   318, 17166,\n",
      "          780,   340, 44389,   287,  1295,   290,   442,  2171,   345,    11,\n",
      "          523,   345,   423,   284,   307,  3131, 30257,   546, 10829, 11685,\n",
      "          618,   345,   651,  3024,   290,  3105,   534,  3842,  2974,   284,\n",
      "          262,   966,   810,   345,   460,  2652,  5894,    13,   921,   460,\n",
      "          635,  1234, 35973,  9493,   364,   287,   534, 11029,  6131,   379,\n",
      "         1755,   284,  5894,   606,    13,   198, 19184, 13288,    11,  8033,\n",
      "          540,  7582,    25,  1649,   345, 15488,   345,   761,   257,  7582,\n",
      "         9664,   326,   468,   262,  2694,   284,  7435, 20160,   290,  2948,\n",
      "          340,   422,  1972,   287,    13, 16038,  2853,   959,   357,    83,\n",
      "        20805,   372,     8, 17557,    12, 17005, 19679,   670,  1049,   329,\n",
      "          428,    13,  8696,   298, 19679,   389,   635,  1695,   543,   743,\n",
      "          307,   517,  8033,   540,   475,   389,   635,  4622, 20140,    13,\n",
      "          198,    38, 39695,    25,  2312,  3002,   262, 32375,   287,   534,\n",
      "        38163,   290, 11005,  4894,  2994,   612,    13,  1119,   635,  2948,\n",
      "        30247,   422,  8218,   534,  3952,    64,    14,  4215,  7582, 27409,\n",
      "          290, 28010,   534,  5101,    13,   383,   308, 39695,   815,   423,\n",
      "         3170,   287, 31833,   364,   290,   443,  7465,   393, 49477,    82,\n",
      "          326,  2948,   606,   422,   852, 16318,   572,   534,  2832,   393,\n",
      "          422,  1972,  2626,    13,   770,   561,   307,   845,  4923,   287,\n",
      "         4692,  6193,  3403,    13,   198,  2025, 37696,  1146, 26929,  9353,\n",
      "           25,  1550, 29175, 18051,    11, 50003,   290,  4467,   389,  9257,\n",
      "         6596,   611,   262, 39513,   290,  9353,   389, 20076,   296,  1146,\n",
      "          662,    12, 22019,  1079,    13,   198,  1890,  1672,    11,   314,\n",
      "          898,   257,  5166,   286, 43047,  4992, 11424,   501, 16627,   641,\n",
      "           11,  3402,  2029,    13,   198,   464, 11424,   501,   285, 34978,\n",
      "          423,   257, 35973, 35940,   351, 11562,   344,   319,   262, 18057,\n",
      "          290, 42186,   276, 32806,   319,   262,   736,   286,   262,  1021,\n",
      "          329,  3131, 23125,    13,   383,  7582,   318,   257, 15787, 36114,\n",
      "        35158,    11,  8033,   540,    11,  4047, 20923, 17557,    12, 17005,\n",
      "          513,    12, 29289, 25508,  2853,   959, 21119,  5330,  7582,   326,\n",
      "          318, 20076,   296,  1146, 26929,   329,  1365, 50003,    13,   383,\n",
      "          308, 39695,   423, 27468, 22656,   269, 45457,   379,   511,  2779,\n",
      "          543,   345,   460, 31833,   866,   284,  1394,   262,  2344,   503,\n",
      "           11,   287,  3090,   284,   281, 22324, 15050,    13,  1318,   447,\n",
      "          247,    82,   635,   257,  3621, 15980,   269,  8589, 23016,   326,\n",
      "          345,   460, 31833,   284,  1394,   262,   285,  2621,   422, 29612,\n",
      "          572,   534,  1021,    13,   198,  7279, 17966,    25, 14576, 48642,\n",
      "          357, 16375,    39, 18320,    13,   785,     8,  8155,   428,   285,\n",
      "        34978,   351,   465,   898,  5153,    13,   198,  6943, 22623, 42016,\n",
      "         2052,   198,    12, 46835, 48659,   285,  2621,  7582,   198,    12,\n",
      "        46835, 48659,   285, 34978,   198,    12,   285,  2621, 19679])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n"
     ]
    }
   ],
   "source": [
    "for token in tokenized_dataset:\n",
    "    print(token['input_ids'])\n",
    "    print(token.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from projector import Combiner\n",
    "# c = Combiner(10,10).to(device)\n",
    "\n",
    "# c.in_proj1.weight.device\n",
    "\n",
    "# cm = torch.nn.ModuleList([Combiner(10, 10) for _ in range(12)]).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/afs/cs.wisc.edu/u/r/i/riyad/Desktop/hybrid-model/hybrid/hybrid_model.py:35\u001b[0m, in \u001b[0;36mHybridModel.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     32\u001b[0m mamba_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba_model\u001b[38;5;241m.\u001b[39mlayers\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Pass through word and position embeddings\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m trans_t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m trans_p_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_model\u001b[38;5;241m.\u001b[39mwpe(torch\u001b[38;5;241m.\u001b[39mtensor([[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]]))\n\u001b[1;32m     37\u001b[0m trans_input_emb \u001b[38;5;241m=\u001b[39m trans_t_emb \u001b[38;5;241m+\u001b[39m trans_p_emb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "for param in transformer_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in mamba_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model = HybridModel(transformer_backbone, mamba_backbone,device) \n",
    "\n",
    "# model.to(device) ? unfreeze : not\n",
    "model.train()\n",
    "\n",
    "trainer_params = []\n",
    "for combiner in model.combiners.parameters():\n",
    "    trainer_params.append(combiner)\n",
    "\n",
    "for spliter in model.splitters.parameters():\n",
    "    trainer_params.append(spliter)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "for e in range(3):\n",
    "    # shuffle the dataset\n",
    "    dataset.set_epoch(e)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i == 5:\n",
    "            break\n",
    "        #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        outputs = model(input_data = batch['input_ids'])\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.__getitem__ of IterableDataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
       "    n_shards: 23781\n",
       "})>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "  0%|          | 0/5 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language_score': tensor([0.9335, 0.9738, 0.9514, 0.9592, 0.8248, 0.9719, 0.8839, 0.9658, 0.9717,\n",
      "        0.8748, 0.9487, 0.9108, 0.9771, 0.8554, 0.9519, 0.9758],\n",
      "       device='cuda:0'), 'token_count': tensor([539, 391, 119, 658,  71, 192, 352, 391, 796, 513, 109, 104, 294, 207,\n",
      "        194, 629], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#t = transformer_tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, return_tensors= \"pt\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/afs/cs.wisc.edu/u/r/i/riyad/Desktop/hybrid-model/hybrid/hybrid_model.py:33\u001b[0m, in \u001b[0;36mHybridModel.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m mamba_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba_model\u001b[38;5;241m.\u001b[39mlayers\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Pass through word and position embeddings\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m trans_t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m trans_p_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_model\u001b[38;5;241m.\u001b[39mwpe(torch\u001b[38;5;241m.\u001b[39mtensor([[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]]))\n\u001b[1;32m     35\u001b[0m trans_input_emb \u001b[38;5;241m=\u001b[39m trans_t_emb \u001b[38;5;241m+\u001b[39m trans_p_emb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "#dataloader = DataLoader(tokenized_dataset,batch_size= 32, collate_fn=DataCollatorForLanguageModeling(transformer_tokenizer,mlm=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.splitters[0].out_proj1.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256, 50256,  ...,   285,  2621, 19679],\n",
       "        [50256, 50256, 50256,  ...,  1708,  6050,    30],\n",
       "        [50256, 50256, 50256,  ...,   286, 11278,    13],\n",
       "        ...,\n",
       "        [50256, 50256, 50256,  ...,    13,  1157, 22199],\n",
       "        [50256, 50256, 50256,  ...,    13,   447,   251],\n",
       "        [50256, 50256, 50256,  ...,   329,  3555,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "  0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['If you go winter backpacking, snowshoeing, or mountaineering, you are going to want to bring at least one pair of mittens. Some people will even bring two pairs in case one gets wet. Personally, I bring a pair of mittens for warmth and a pair of gloves for added dexterity.\\nWhen selecting mittens and gloves for winter conditions, here are a couple of must-have features that you should look for:\\nRemovable liners: You need a mitten or glove system that has liners and a removable shell so you don’t overheat and sweat. Sweat in winter is nasty because it freezes in place and chills you, so you have to be extra vigilant about removing layers when you get hot and slow your activity levels to the point where you can stay dry. You can also put removable liners in your sleeping bag at night to dry them.\\nWaterproof, breathable shell: When you sweat you need a shell fabric that has the ability to vent moisture and prevent it from getting in. Higher denier (tougher) Gore-Tex shells work great for this. EVent shells are also available which may be more breathable but are also slightly heavier.\\nGauntlets: These cover the veins in your wrists and eliminate heat loss there. They also prevent drafts from entering your parka/soft shell sleeves and chilling your arms. The gauntlets should have built in tighteners and leashes or tethers that prevent them from being blown off your hands or from getting lost. This would be very dangerous in cold weather conditions.\\nAnatomically curved fingers: On thicker gloves, dexterity and comfort are greatly improved if the palms and fingers are anatomically pre-curved.\\nFor example, I own a pair of Outdoor Research Cornice Mittens, shown above.\\nThe Cornice mittens have a removable liner with fleece on the palm and lofted insulation on the back of the hand for extra warmth. The shell is a seam taped waterproof, breathable, highly durable Gore-Tex 3-layer 330 denier Cordura shell that is anatomically curved for better dexterity. The gauntlets have elastic locking cinches at their base which you can tighten down to keep the wind out, in addition to an idiot cord. There’s also a nice wrist cinch strap that you can tighten to keep the mitten from slipping off your hand.\\nDisclosure: Philip Werner (SectionHiker.com) purchased this mittens with his own funds.\\nMost Popular Searches\\n- gore tex mitten shell\\n- gore tex mittens\\n- mitten shells', 'The Roman Catholic Church loves them some babies, babies from incest, babies from rape, babies from unmarried consenting adults, doesn’t matter. They’re all good. But their view on parents is whacked out. For example, when a teacher and her husband turned to in vitro fertilization, the church canned her despite exemplary performance. Not sure how this will play out legally, because it turns out churches get a special pass on discrimination unavailable to any other employer:\\n(HuffPo) — The U.S. Supreme Court ruled unanimously in January that religious workers can’t sue their employers for job discrimination because anti-discrimination laws allow for a “ministerial exception.” But the justices failed to define who was and who wasn’t a religious employee.”The Supreme Court didn’t give us a kind of neat little on-off test as to who’s a minister and who isn’t,” said Rick Garnett, associate dean and professor of law at Notre Dame Law School.\\nIn a similar case in Ohio, a federal judge last month gave the go-ahead for a trial in a lawsuit against the Archdiocese of Cincinnati by a parochial school teacher who was fired after she became pregnant through artificial insemination, which the church is also against. The archdiocese fired Christa Dias in 2010, saying the single woman violated church doctrine.\\nU.S. District Judge Arthur Spiegel said in his March 29 ruling that the ministerial exception did not apply because Dias was a non-Catholic computer teacher with no role in ministering or teaching Catholic doctrine.\\nOK, fine. So what would stop a “church” from setting up shop, using neo-confederate theology, and discriminating against whatever minorities, they happened to feel superior to? What would stop a mosque or a synagogue from following suit?', 'Event planning is a challenging but rewarding job that requires a very special kind of person. It requires a keen eye for detail, meticulous organization, and saint-like patience. It can be tedious, unpredictable, and even dangerous – but there’s an app for that. Several.\\nDespite the inherent advantages of mobile technologies built specifically for event planning, a recent survey showed that 59 percent of event professionals do not use mobile apps in their event planning, but that 53 percent of those were interested in getting one. That’s a sure sign of a market on the brink of explosion.', 'Too often, there is panic when an organization experiences an executive vacancy.\\nGiven that most nonprofits are strapped for resources, board members face the real possibility they might have to step in and do that work because there is no surplus internal staff capacity.\\nBoards, then, often feel forced to consider short-term questions. How quickly can we fill this position? Who do we already know in this field? Can we quickly post the current job description? Can we fill it internally with someone who already knows everything?\\nBut the exit of a senior staffer should be a strategic choice point – an opportunity to look forward to future needs without having to consider the presence, capacity and limitations of the departing executive.\\nThis is the prime time to assess an organization’s business model and staffing to determine what is needed to advance the organization’s mission.\\nFor some nonprofits, a professional interim executive can play the key role in that succession planning by providing the opportunity to disconnect from the previous executive.\\nThe Presbyterian Church, for example, has long insisted that a retiring senior pastor be followed by an interim preacher, from outside, to break the congregation’s ties to the former pastor.\\nThis exercise serves to open up the church to what comes next.\\nA nonprofit with a skilled interim executive in place can disconnect in a similar fashion from its former leader and thoughtfully assess next steps, all while the daily operation of the organization forges ahead.\\nMoving too quickly to fill a vacancy can result in the candidate who most resembles the departing executive or who is his or her polar opposite.\\nSuch a selection often is a reaction to previous skills and personality rather than an objective assessment of what’s most needed.\\nSometime boards select a prominent retired business leader as the interim, then congratulate themselves on a distinguished placement.\\nBut without prior nonprofit experience, even an experienced corporate executive can have difficulty adjusting to work life without personal staff, which can mean doing their own filing and PowerPoint presentations, ordering their own supplies, making high-level presentations and then stopping at the grocery store to pick up napkins for a fundraising reception.\\nIt is equally important to consider the impact of asking an internal staff member to serve as the interim.\\nThat person likely is already over-burdened, and being asked to assume additional work might not be welcome, even if these are higher level responsibilities.\\nAnd an internal staff interim can potentially create negative impact after the successor executive is hired.\\nIs he or she a candidate for the position? Will she be paid more? When the interim period is over, will she be “demoted” back to her former position? Why is it she could be capable of assuming interim executive duties but not qualified for the permanent position?\\nContracting with a professionally trained and experienced interim executive can be smart succession planning.\\nAn experienced interim can keep the staff engaged, donors interested and the board appropriately involved during the search for a permanent hire.\\nUltimately, an interim executive can provide the board of directors with the opportunity to determine how best to move into a successful future.\\nKathy Ridge is the founder of Edvance Consulting Group, a North Carolina-based firm that provides education and nonprofit consulting and interim executive services.', 'Here are the stats for Midlifewanderlust1965\\nViews 460 (A New High)\\nFollowers 82 (+13)\\nTotal Tweets 10,715\\nIt is going to be a quiet April. There will be some day trips while planning ahead for September.\\nUntil the next post stay safe, stay healthy and keep smiling.', \"My brother's wedding is approaching fast, coming up this Saturday! For his special occasion I decided to rent a lens! SOO after much deliberation on what I wanted to try out I finally decided on a Nikon 17-55mm f/2.8G AF-S DX IF-ED, and boy do I LOVE it!! I was a little leery about renting a lens and really hoping and praying that my experience would go well...I put in my order on a Sunday night and requested it to be shipped right away so it arrived the following Wednesday! So far so good! I am absolutely LOVING this lens!! The bad part is I think I will actually be sad when I have to send it back...Now I'm trying to figure out how to convince the Hubby to buy me one ;)\\nSo I thought I would share some of my deserted country road finds! You can see the awesome detail by clicking on the picture!!\", 'Hospitals and Healthcare Systems\\nAt the heart of any healthy community is a healthy hospital that cares as much about its environment, employees, and neighborhood, as it does its patients. RRS assists hospitals and health systems in minimizing waste generation, costs and risks through sustainability program planning and management, policy development, compliance oversight, and training and education.\\n|Waste/Recycling Program Planning & Management||Service & Product Procurement|\\n|Food Waste Reduction & Recovery Planning||Contract Execution & Management|\\n|Waste Audit/Assessment||Policy, Procedure & Guideline Development|\\n|Compliance Expertise, Tracking & Reporting||Training Development & Facilitation|\\n|Chemical Inventory & Safety Data Sheet Management||Hazardous/Regulated Medical Waste Program Management|\\n|Green Team Facilitation||Communications and Outreach|\\nHenry Ford Health System: Conducted baseline waste/compliance assessments, developed system-wide policies/procedures and training program.\\nHenry Ford Hospital: Implemented resource management program including recycling, hazardous materials and regulated medical waste management, compliance audits and training.\\nMercy Medical Center: Conducted baseline assessments of recycling program, hazardous waste, regulated medical waste, green products, education and alternative energy.\\nMedline Industries: conducted surveys and waste sorts in hospital operating rooms to recommend best practices for compliance, waste minimization and disposal cost reduction.\\nTrinity Health: Resource management of multiple facilities including coordinating pickup, transport and recycling/disposal, as well as negotiating a bundled service contract.\\nBeaumont Health: Implemented food waste composting program following in-depth review of i-building procedures, foodservice product purchasing records, service vendors and cost options.', '- About Us\\n- Global Vision\\n- News and Events\\n- Ways to Help\\n- Advancing the Human Spirit\\nDennis Felty Receives the First PAR Innovation Award\\nDennis Felty, Keystone Human Services’ Founding President, received the first Pennsylvania Advocacy and Resources for Autism and Intellectual Disabilities (PAR) Innovation Award. The award was presented during PAR’s 2013 Solutions Conference on November 20.\\nDennis has dedicated his entire professional life to the belief that people have a deep obligation to serve as change agents, sharing their professional experience with others individually and within the public policy and service delivery processes because the welfare of all people is vested in the welfare of each individual. He holds the belief that people must continually challenge themselves and their organizations to find creative and innovative ways to discover, honor, and value the inherent worth of all people. His vision and Keystone Human Service’s vision is to create an environment where all people, regardless of background and ability, can grow, make choices, and be valued and contributing members of the community.\\nHe and Keystone are dedicated to innovating new and meaningful social forms and structures that have value to all people through the acceptance and understanding of the great diversity within the human condition. These innovations and cutting edge concepts include William Penn Human Services, the Adult Community Autism Program (ACAP), work with children and adults with disabilities in Moldova, Beslan, and South Africa, the conceptualization of the Comprehensive Residential Procedure Code, and the Margin Factor.\\nDennis has been involved in human services since 1968 and was among the original members of PAR. He has been active in shaping PAR’s mission, vision, and strategic direction and has received PAR’s Leadership and Humanitarian awards in past years. He has also served on numerous other national, state, and community Boards and task forces, and he has been a member of the Clinton Global Initiative since 2007.', 'If we could start over from scratch in building our public education system, how would we do it? In New Orleans, that question was far from academic in 2005 after the devastation of Hurricane Katrina. In fact, the question was literal; at the end of the 2005-6 school year, the city only had six schools in operation. Before Katrina, they may as well have had only six, as they had one of the worst-performing school districts in the nation. As one person relates in this Reason TV video, one school had a valedictorian who could not pass a graduation exam in six attempts despite getting straight As in high school.\\nNew Orleans had a choice in creating a new school system — and choice became a first principle, as Nick Gillespie explains:\\nBefore hurricane Katrina ravaged the city in 2005, New Orleans had one of the worst performing public school districts in the nation. Katrina forced nearly a million people to leave their homes and caused almost $100 billion in damages. To an already failing public school system, the storm seemed to provide the final deathblow. But then something amazing happened. In the wake of Katrina, education reformers decided to seize the opportunity and start fresh with a system based on choice.\\nToday, New Orleans has the most market-based school system in the US. Sixty percent of New Orleans students currently attend charter schools, test scores are up, and talented and passionate educators from around the country are flocking to New Orleans to be a part of the education revolution. It’s too early to tell if the New Orleans experiment in school choice will succeed over the long term, but for the first time in decades people are optimistic about the future of New Orleans schools.\\nThe key attributes are competition, parental choice, investment, and an end to the union deathgrip on New Orleans schools that kept children locked into failing schools and failing classrooms. Parents in New Orleans have hope now that their children will get educated rather than baby-sat, and that will provide a renaissance of its own to a city struggling to get back on its feet.\\nOtherwise, we’ll end up with this, courtesy of Bob Ewing at the Daily Caller:\\nEveryone knew OSP would be a bargain. DC has among the highest spending per pupil in the nation. At a conservative estimate of $17,542, the public schools spend over $10,000 more per child than the $7,500 spent through the scholarship program.\\nBut would OSP achieve measureable results?\\nThe answer is a resounding yes. Previous studies by Wolf showed an improvement in academic performance, to the point that a student participating in OSP from kindergarten through high school would likely be 2 ½ years ahead in reading. The key finding in this final round of research, Wolf told us, was the graduation rates. OSP dramatically increases prospects of high-school graduation.\\nWolf pointed to research showing that high-school diplomas significantly improve the chance of getting a job. And dropouts that do find employment earn about $8,500 less per year than their counterpoints with diplomas. Further, each graduate reduces the cost of crime by a stunning $112,000. Cecelia Rouse, an economic advisor to President Obama, found that each additional high school graduate saves the country $260,000.\\nSimply put, OSP has a profoundly positive effect not just on students, but on the city and the country as a whole.\\nSo when it came time for Congress to reauthorize OSP, it would seem to be a no-brainer: Expand the program.\\nInstead, they killed it.\\nOf course. They haven’t had a Katrina to refocus Congress on what ails education; instead, they’re acting in thrall to the teachers union. Be sure to read it all; it’s as depressing as the Reason TV video is uplifting.', 'Explanation dashboard symbols\\nIn this guide all symbols and elements from the Survey Anyplace dashboard will be explained.\\n- The navigation menu\\n- All recent activity to check which survey got a response recently\\n- Number of responses over different periods\\n- The dashboard to manage all of your surveys\\n- Click here to create a new survey/quiz.\\n- You probably clicked on this button to wind up here.\\n- If you want to go to the dashboard and have an overview of all your surveys, click on this \\'Dashboard\\' button\\n- Via \\'Survey\\' you go to the editor tabs of the survey you last edited.\\n- Via \"Results\\' you go to the results of the survey you last edited\\n- Via \\'Upgrade/invoices\\' you can manage your account and licenses.\\n- User Management allows you to work together with your colleagues. Here you can determine the team settings.\\n- In Account you an change your password and complete your profile.\\n- When you click on \\'Logout\\' you will be logged out.\\n- This is the title you gave your survey/quiz. When you click on the title, you\\'ll be able to edit that specific survey/quiz.\\n- This is the email of the person who made the survey/quiz. In most cases this will be your e-mail address.\\n- This is the URL you gave your survey/quiz. When you click on the URL, it\\'ll take you to the survey/quiz you made. When you take the survey/quiz via this way, you\\'re not in the preview mode and your answers will count as a response.\\n- This is the number of responses the survey/quiz obtained\\n- This is the date of the first response.\\n- This is the date of the last response.\\n- By clicking on the \\'Share\\' button you go to the tab where you\\'ll find the link, QR code and iFrame to distribute your survey\\n- By clicking on the \\'Results\\' button you go to the tab to analyze your results\\n- By clicking on the \\'Copy\\' button, you copy your survey. Mind that business rules and skip logic won\\'t be copied.\\n- By clicking on the \\'Delete\\' button, you delete your survey. ATTENTION: this is irreversible.\\n- This switch allows you to make your survey active or inactive. When the survey is inactive, the survey won\\'t be found when you enter the survey URL.', 'Everyday countless people trim their pets toenails without knowing how to do it properly and cause them pain. Often they cut too close and into “the quick” which causes both pain and bleeding for the pet.\\nSome dogs don’t like their nails trimmed and give the owner a hard time, causing the pet stress and making it very difficult to do.\\nWe offer this service for you to make it easier on you and your pet.\\nCanine Pedicure $15\\nFeline Pedicure $15', 'Previous Demo (World Gone Sour) | Next Demo (NBA Jam On Fire Edition)\\nWorld Gone Sour is the fortieth episode of Demo Fridays done by Stephen. It also features Mallory.\\nAd blocker interference detected!\\nWikia is a free-to-use site that makes money from advertising. We have a modified experience for viewers using ad blockers\\nWikia is not accessible if you’ve made further modifications. Remove the custom ad blocker rule(s) and the page will load as expected.', '20 September 2016\\nThere has been a lot hype given to the development of Hyderabad city since the TRS government was formed. KCR’s government has set many expectations among the people about the beautification and cleanliness that the city is going to grasp. But now the horrible situation in the city has exposed the hollowness of the promises given by the Telangana Government. Actually the city turned from bad to worse. The entire city of Hyderabad has turned into a muddy well. The inefficiently designed drainage system could not cope up with the recent heavy rains and the roads are totally turned into ruins. Whether it is old city or Banjara Hills or Hi-Tech City, no matter where, everything looks the same.\\nOn the other hand, the minister of urban development, Mr. KTR wants to bring-up the city to make another Dallas. He also announced a 100-day pilot program to beautify the city. Of course, this order was passed just before the GHMC elections. After that elections are gone and the beautification plan also gone.\\nNo wonder, people are now questioning KTR directly through letters and tweets. Senior Congress leader and former MP V Hanumantha Rao demanded that KTR resign immediately for failing to maintain proper roads in the city. All the opposition leaders have already started questing the government. People are waiting to see where their city roads are going to lead them in the future.', 'Glow-In-The-Dark Glow Gloves\\nGet glowing this Halloween and wear Glow-In-The-Dark Glow Gloves! These spooky glowing gloves are perfect to wear trick or treating, they are a great way to keep an eye on your ghouls. If you are looking for an awesome safety item, these glow gloves are perfect. Glow-In-The-Dark Glow Gloves put smiles on faces & are truly a great way to get noticed.\\nAll you need to activate the awesome glow is sun light! Charge those bad boys in the sun, then wear until the glowing stops!\\nNOTE: This item is sold in pairs. Buying 1 piece means you are buying 1 pair. Example - order 12 pieces to receive 24 individual gloves.\\nOne size fits most.\\nGlow-In-The-Dark Glow Gloves do not include or need batteries.\\nPackaging: Retail ready, individual poly bags, 12 pairs per inner box - 144 pairs per carton.\\nWeight: 0.11 LB', 'RFL chief executive Nigel Wood (pictured) has congratulated Featherstone Rovers for clinching a second successive Co-operative Championship League Leaders Shield.\\nRovers finished three points clear of closest rivals Leigh Centurions after sealing top spot in the table by hammering Super League bound Widnes Vikings 44-4 on Sunday.\\nIt extended Featherstone’s unbeaten league run to 19 matches. Their solitary league defeat this season was at Leigh on the opening day.\\n“On behalf of the RFL I would like to congratulate everyone at Featherstone Rovers for their success in winning the 2011 Championship League Leaders Shield, an achievement that everyone at the club should be proud of,” said Wood.\\n“Featherstone have been one of the most consistent sides in the Co-operative Championship over the past two seasons and their success in retaining the Leaders shield this season is testimony to the quality of their on-field performances in 2011.”', 'I was kindly tagged by notsomoderngirl and thought I would try it. Bear with me, fairly new to blogging and tags! 🤓\\nFlagrate: Writing Charm\\nA book whose theme you found interesting but would like to re-write.\\nI have to pick The Smaller Evil by Stephanie Kuehn. I found the cult like group fascinating but I think I would’ve enjoyed this book a lot more if the last 1/4th was clearer and not quite so confusing. I was enjoying the book until that point and it just ended on a low note for me.\\nAlohomora: Unlocking Charm\\nThe first book in a series that got you hooked.\\nThe Girl with the Dragon Tattoo by Steig Larsson. I absolutely loved this book. It took a little bit to get going but once it did, I was hooked!\\nAccacio: Summoning Charm\\nA book you wish you could have right now.\\nI would love to have Slade House by David Mitchell right now. It’s already available but I just don’t have the funds currently. As soon as I do, rest assured it will be bought! I need to read The Bone Clocks first as they’re supposed to be tied together in some way.\\nAuada Kedaura: Killing Curse\\nA killer book.\\nThe Creeper Man by Dawn Kurtagich. I LOVED this book. So in those terms, it’s a killer book! It also has horror, mystery, death, themes so I suppose that makes it killer too.\\nConfundo: Confusing Charm\\nA book that was really confusing.\\nMy pick for this one is House of Leaves by Mark Z. Danielewski. I’ve attempted to read this before and gave up about 1/4 the way through. I was confused about a lot of it but I was quite a bit younger. I’m hoping now that I’m a bit older, I can work my way through it. It seems like such an amazing book!\\nSectumsempra: Dark Charm\\nA dark and twisted book.\\nHorrorstor by Grady Hendrix. Going into this book, I didn’t have high hopes. It’s rare a book (or movie) can actually give me the creeps. This book achieved that! I was pleasantly surprised!\\nAparecium: Revealing Charm\\nA book that surprised you in a great way.\\nOh my, did this book blow me away! I went into it knowing very little, even thinking it was from the horror genre. I won’t go in to too much detail because of spoilers, but this book will stay with me for a very long time!\\nI’m new at WordPress so I don’t want to tag too many people, not knowing who has already done it. Feel free to do it skip if you’d like!\\nThanks for reading!'], 'id': ['<urn:uuid:12ecc351-55d2-44b1-822a-58c2b800b76d>', '<urn:uuid:0a6a94f8-9639-45fc-aa60-142a086557de>', '<urn:uuid:d55aff8f-20d4-4961-b576-bb2f21f02d4a>', '<urn:uuid:ae3672f6-913a-41a2-aa9f-999046c0c446>', '<urn:uuid:5db50e26-c8ac-4214-aa17-4d54f5019bd9>', '<urn:uuid:27922a6e-4dab-485e-b6f7-abcf7c1cd582>', '<urn:uuid:012b734e-1133-4c1d-bb39-a4822ca69abf>', '<urn:uuid:b797a948-e2ec-4ad1-a777-3d75216a4c19>', '<urn:uuid:d30597b1-3904-424e-9b15-4d903034a212>', '<urn:uuid:8d1ee0e7-30d0-413d-a25a-e37bff1fe1b5>', '<urn:uuid:bac995cb-23c9-4dc9-b6ce-1bd36b1f62e7>', '<urn:uuid:c38dbe7c-84e3-4465-a6be-471ee44fbd3e>', '<urn:uuid:a5bdb2d0-483f-46d4-86e6-8b86f068ea53>', '<urn:uuid:90994fe9-0802-4a8c-b6be-c4b3894b8653>', '<urn:uuid:951afe31-5b81-4dfc-a7ae-48bd343becab>', '<urn:uuid:e68fcd96-946f-4010-ad66-274f949659ce>'], 'dump': ['CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26', 'CC-MAIN-2017-26'], 'url': ['https://sectionhiker.com/winter-gear-outdoor-research-cornice-gore-tex-mittens/', 'https://freethoughtblogs.com/zingularity/2012/04/26/teacher-fired-by-catholic-school-for-getting-pregnant-the-wrong-way/', 'http://www.bpwrap.com/tag/events/', 'https://pj.news.chass.ncsu.edu/2011/04/15/case-interim-executive-director/', 'https://midlifewanderlust1965.wordpress.com/2013/04/01/statistics-march-2013/', 'http://amdunbarranch.blogspot.com/2010/03/testing-new-lens-and-loving-it.html', 'https://recycle.com/our-services/industries-we-serve/hospital-healthcare-system/', 'http://www.keystonehumanservices.org/keystone-news/dennis-felty-receives-the-first-par-innovation-award.php', 'http://hotair.com/archives/2010/07/08/reason-tv-the-education-renaissance-of-katrina/', 'https://surveyanyplace.com/docs/explanation-dashboard-symbols/', 'http://dayandeveningpetclinic.com/services/nail-trims/', 'http://stephengeorg.wikia.com/wiki/Spelunky_-_Demo_Fridays', 'http://www.teluguroommate.com/politics/hyderabad-rains-and-roads.html', 'https://bongoflashers.com/products/glow-in-the-dark-glow-gloves', 'http://www.pontefractandcastlefordexpress.co.uk/sport/rugby-league/featherstone-rovers/rovers-congratulated-by-rfl-chief-wood-1-3743183', 'https://littlelifelibrary.wordpress.com/2017/01/04/harry-potter-book-tag/'], 'date': ['2017-06-28T19:05:48Z', '2017-06-23T08:44:04Z', '2017-06-23T08:48:36Z', '2017-06-29T14:19:46Z', '2017-06-29T14:11:13Z', '2017-06-24T03:42:20Z', '2017-06-28T19:04:50Z', '2017-06-23T08:33:02Z', '2017-06-29T14:27:05Z', '2017-06-28T19:11:57Z', '2017-06-23T08:22:20Z', '2017-06-24T03:40:34Z', '2017-06-23T08:38:23Z', '2017-06-23T08:25:44Z', '2017-06-23T08:34:21Z', '2017-06-23T08:24:24Z'], 'file_path': ['s3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128323730.30/warc/CC-MAIN-20170628185804-20170628205804-00302.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128329344.98/warc/CC-MAIN-20170629135715-20170629155715-00382.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128329344.98/warc/CC-MAIN-20170629135715-20170629155715-00382.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320215.92/warc/CC-MAIN-20170624031945-20170624051945-00102.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128323730.30/warc/CC-MAIN-20170628185804-20170628205804-00302.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128329344.98/warc/CC-MAIN-20170629135715-20170629155715-00382.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128323730.30/warc/CC-MAIN-20170628185804-20170628205804-00302.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320215.92/warc/CC-MAIN-20170624031945-20170624051945-00102.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz', 's3://commoncrawl/crawl-data/CC-MAIN-2017-26/segments/1498128320040.36/warc/CC-MAIN-20170623082050-20170623102050-00021.warc.gz'], 'language': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'], 'language_score': tensor([0.9335, 0.9738, 0.9514, 0.9592, 0.8248, 0.9719, 0.8839, 0.9658, 0.9717,\n",
      "        0.8748, 0.9487, 0.9108, 0.9771, 0.8554, 0.9519, 0.9758]), 'token_count': tensor([539, 391, 119, 658,  71, 192, 352, 391, 796, 513, 109, 104, 294, 207,\n",
      "        194, 629])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m     10\u001b[0m t \u001b[38;5;241m=\u001b[39m transformer_tokenizer(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:687\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    684\u001b[0m         use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n\u001b[1;32m    690\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(params=trainer_params , lr=1e-5)\n",
    "for epoch in range(3):\n",
    "    dataset.set_epoch(epoch)\n",
    "    for i, batch in enumerate(tqdm(batched_dataset, total=5)):\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        print(batch)\n",
    "        t = transformer_tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, return_tensors= \"pt\")\n",
    "        outputs = transformer_backbone(t['input_ids'])\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
