{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model\n",
    "\n",
    "1. Inference  \n",
    "  a. Infer from a pre-trained Mamba and a pre-trained transformer model.  \n",
    "  b. Infer using subset of layers of the models.   \n",
    "  c. Pass layer output thorough intermediate linear layers.   \n",
    "  d. Combine intermediate linears and project again   \n",
    "2. Training  \n",
    "  a. Freeze the model and train linears  \n",
    "  b. Train both pretrained and linear.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Inferring from Pretrained Mamba and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey how are you doing?\\n\\nI'm so glad you're here.\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "mamba_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n",
    "\n",
    "out = mamba_model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_inputs: {'input_ids': tensor([[10814,   703,   389,   345,  1804,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Hey how are you doing?\n",
      "\n",
      "I'm doing a lot of research on the internet and I'm not sure if I'm doing it right or not. I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing it right or not.\n",
      "\n",
      "I'm trying to find out what's going on with the internet and I'm not sure if I'm doing\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "\n",
    "prompt = \"Hey how are you doing?\"\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to('cpu')\n",
    "print('model_inputs:', model_inputs)\n",
    "\n",
    "model_name = 'EleutherAI/gpt-neo-125M'\n",
    "trans_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "generated_ids = trans_model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(trans_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Infer Using Model Layers Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input emb shape torch.Size([1, 6, 768])\n",
      "Output of layers: torch.Size([1, 6, 768])\n",
      "Token with max probability: , to you doing?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Get the transformer model layers\n",
    "layers = trans_model.transformer.h\n",
    "\n",
    "prompt = \"Hey how are you doing?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to('cpu')\n",
    "\n",
    "# Initialize the input\n",
    "input_data = model_inputs['input_ids']\n",
    "\n",
    "# Pass through word and position embeddings\n",
    "t_emb = trans_model.transformer.wte(input_data)\n",
    "p_emb = trans_model.transformer.wpe(torch.tensor([[i for i in range(input_data.shape[1])]]))\n",
    "input_emb = t_emb + p_emb\n",
    "print(\"Input emb shape\", input_emb.shape)\n",
    "\n",
    "# Pass the input through each layer individually\n",
    "for i, layer in enumerate(layers):\n",
    "    input_emb = layer(input_emb)[0]\n",
    "\n",
    "print(f\"Output of layers: {input_emb.shape}\")\n",
    "    \n",
    "# Get the output of the last layer\n",
    "ln_output = trans_model.transformer.ln_f(input_emb)\n",
    "output = trans_model.lm_head(ln_output)\n",
    "\n",
    "# Take the token with the maximum probability\n",
    "max_prob_token = torch.argmax(output, dim=-1)\n",
    "\n",
    "# Decode the token to text\n",
    "decoded_token = tokenizer.decode(max_prob_token[0], skip_special_tokens=True)\n",
    "print(\"Token with max probability:\", decoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaForCausalLM(\n",
      "  (backbone): MambaModel(\n",
      "    (embeddings): Embedding(50280, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x MambaBlock(\n",
      "        (norm): MambaRMSNorm(768, eps=1e-05)\n",
      "        (mixer): MambaMixer(\n",
      "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
      "          (act): SiLU()\n",
      "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
      "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_f): MambaRMSNorm(768, eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mamba_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input emb shape torch.Size([1, 6, 768])\n",
      "Output of Mamba layers: torch.Size([1, 6, 768])\n",
      "Mamba Token with max probability: , about you?\"?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n",
    "\n",
    "# Get the Mamba model layers\n",
    "mamba_layers = mamba_model.backbone.layers\n",
    "\n",
    "# Pass through word embeddings\n",
    "input_embeds = mamba_model.backbone.embeddings(input_ids)\n",
    "print(\"Input emb shape\", input_embeds.shape)\n",
    "\n",
    "# Pass the input through each layer individually\n",
    "hidden_states = input_embeds\n",
    "for mixer_block in mamba_layers:\n",
    "    hidden_states = mixer_block(hidden_states)\n",
    "\n",
    "print(f\"Output of Mamba layers: {hidden_states.shape}\")\n",
    "\n",
    "# Get the output of the norm layer\n",
    "norm_output = mamba_model.backbone.norm_f(hidden_states)\n",
    "mamba_output = mamba_model.lm_head(norm_output)\n",
    "\n",
    "# Take the token with the maximum probability\n",
    "mamba_max_prob_token = torch.argmax(mamba_output, dim=-1)\n",
    "\n",
    "# Decode the token to text\n",
    "mamba_decoded_token = tokenizer.decode(mamba_max_prob_token[0], skip_special_tokens=True)\n",
    "print(\"Mamba Token with max probability:\", mamba_decoded_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Pass Layer Output Through Intermediate Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input emb shape torch.Size([1, 6, 768])\n",
      "Output of Mamba layers: torch.Size([1, 6, 768])\n",
      "Mamba Token with max probability: �st form Goneliness}{~\n"
     ]
    }
   ],
   "source": [
    "class IntermediateLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(IntermediateLinear, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features, out_features)\n",
    "        self.linear2 = torch.nn.Linear(out_features, in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.linear1(x))\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n",
    "\n",
    "# Get the Mamba model layers\n",
    "mamba_layers = mamba_model.backbone.layers\n",
    "\n",
    "# Pass through word embeddings\n",
    "input_embeds = mamba_model.backbone.embeddings(input_ids)\n",
    "print(\"Input emb shape\", input_embeds.shape)\n",
    "\n",
    "# Create an intermediate layer\n",
    "interm = IntermediateLinear(input_embeds.shape[-1], input_embeds.shape[-1] * 2)\n",
    "\n",
    "# Pass the input through each Mamba layer and then the intermediate layer\n",
    "hidden_states = input_embeds\n",
    "for mixer_block in mamba_layers:\n",
    "    hidden_states = mixer_block(hidden_states)\n",
    "    hidden_states = interm(hidden_states)\n",
    "\n",
    "print(f\"Output of Mamba layers: {hidden_states.shape}\")\n",
    "\n",
    "# Get the output of the norm layer\n",
    "norm_output = mamba_model.backbone.norm_f(hidden_states)\n",
    "mamba_output = mamba_model.lm_head(norm_output)\n",
    "\n",
    "# Take the token with the maximum probability\n",
    "mamba_max_prob_token = torch.argmax(mamba_output, dim=-1)\n",
    "\n",
    "# Decode the token to text\n",
    "mamba_decoded_token = tokenizer.decode(mamba_max_prob_token[0], skip_special_tokens=True)\n",
    "print(\"Mamba Token with max probability:\", mamba_decoded_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Combine Intermediate Layers and Project Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner and Splitter follow the Manticore model architecture\n",
    "\n",
    "class Combiner(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Combiner two inbound projectors for outputs from two model blocks. \n",
    "    The projected outputs are added in a weighted fashion. The combined output \n",
    "    is passed to the Splitter or LM head.\n",
    "    \n",
    "    The projected output has dimension as the maximum of the two input dimensions.   \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim1, input_dim2):\n",
    "        super(Combiner, self).__init__()\n",
    "        proj_dim = max(input_dim1, input_dim2)\n",
    "        self.in_proj1 = torch.nn.Linear(input_dim1, proj_dim)\n",
    "        self.in_proj2 = torch.nn.Linear(input_dim2, proj_dim)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1_proj = self.in_proj1(x1)\n",
    "        x2_proj = self.in_proj2(x2)\n",
    "        combined = x1_proj + x2_proj\n",
    "        return combined\n",
    "\n",
    "class Splitter(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Splitter is used to split the output of the intermediate combiner \n",
    "    into two parts to be passed to the two model blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim1, input_dim2):\n",
    "        super(Splitter, self).__init__()\n",
    "        proj_dim = max(input_dim1, input_dim2)\n",
    "        self.out_proj1 = torch.nn.Linear(proj_dim, input_dim1)\n",
    "        self.out_proj2 = torch.nn.Linear(proj_dim, input_dim2)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out_proj1(x), self.out_proj2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans input emb shape torch.Size([1, 6, 768])\n",
      "Mamba input emb shape torch.Size([1, 6, 768])\n",
      "Output of combined: torch.Size([1, 6, 768])\n",
      "Hybrid token with max probability: àààààà\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "prompt = \"Hey how are you doing?\"\n",
    "trans_tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "mamba_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "trans_model_inputs = trans_tokenizer(prompt, return_tensors=\"pt\").to('cpu')\n",
    "mamba_inputs = mamba_tokenizer(prompt, return_tensors= \"pt\")\n",
    "\n",
    "# Initialize the input\n",
    "trans_input_data = trans_model_inputs['input_ids']\n",
    "mamba_input_ids = mamba_inputs[\"input_ids\"]\n",
    "\n",
    "# Get the transformer and mamba model layers\n",
    "trans_layers = trans_model.transformer.h\n",
    "mamba_layers = mamba_model.backbone.layers\n",
    "\n",
    "# Pass through word and position embeddings\n",
    "trans_t_emb = trans_model.transformer.wte(trans_input_data)\n",
    "trans_p_emb = trans_model.transformer.wpe(torch.tensor([[i for i in range(trans_input_data.shape[1])]]))\n",
    "trans_input_emb = trans_t_emb + trans_p_emb\n",
    "print(\"Trans input emb shape\", trans_input_emb.shape)\n",
    "\n",
    "mamba_input_embeds = mamba_model.backbone.embeddings(input_ids)\n",
    "print(\"Mamba input emb shape\", mamba_input_embeds.shape)\n",
    "\n",
    "# Create intermediate layers and LM head\n",
    "combiner = Combiner(trans_input_emb.shape[-1], mamba_input_embeds.shape[-1])\n",
    "splitter = Splitter(trans_input_emb.shape[-1], mamba_input_embeds.shape[-1])\n",
    "proj_dim = max(trans_input_emb.shape[-1], mamba_input_embeds.shape[-1])\n",
    "hybrid_lm_head = torch.nn.Linear(proj_dim, trans_model.lm_head.out_features)\n",
    "\n",
    "# Pass the input through each block and intermediate layers\n",
    "combined_emb = trans_input_emb\n",
    "for i in range(12):\n",
    "    trans_input_emb, mamba_input_embeds = splitter(combined_emb)\n",
    "    trans_input_emb = trans_layers[i](trans_input_emb)[0]\n",
    "    mamba_input_embeds = mamba_layers[2*i](mamba_input_embeds)\n",
    "    mamba_input_embeds = mamba_layers[2*i+1](mamba_input_embeds)\n",
    "    combined_emb = combiner(trans_input_emb, mamba_input_embeds)\n",
    "    \n",
    "print(f\"Output of combined: {combined_emb.shape}\")\n",
    "    \n",
    "# No norm layer for now \n",
    "hybrid_output = hybrid_lm_head(combined_emb)\n",
    "\n",
    "# Take the token with the maximum probability\n",
    "hybrid_max_prob_token = torch.argmax(hybrid_output, dim=-1)\n",
    "\n",
    "# Decode the token to text\n",
    "hybrid_decoded_token = trans_tokenizer.decode(hybrid_max_prob_token[0], skip_special_tokens=True)\n",
    "print(\"Hybrid token with max probability:\", hybrid_decoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
